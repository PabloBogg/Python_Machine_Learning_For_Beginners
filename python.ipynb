{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PabloBogg/Python_Machine_Learning_For_Beginners/blob/main/python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PSI9vB140qE"
      },
      "source": [
        "## **PYTHON MACHINE LEARNING FOR BEGINNERS**\n",
        "**Learning from scratch Numpy, Pandas, Matplotlib, Seaborn, Scikitlearn, and TensorFlow for MAchine Learning and Data Science**\n",
        "\n",
        "\n",
        "*To get the Python codes and materials used in this book, please click the link below:*\n",
        "\n",
        "www.aipublishing.io/book-machine-learning-python\n",
        "\n",
        "*The order number is required.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNm1ROM4FNMU"
      },
      "source": [
        "# **1**\n",
        "# **Introduction and Environment Set Up**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eBgFTT46rY6"
      },
      "source": [
        "**1.3.4. Using Google Colab Cloud Environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZijg8Xm6y4m"
      },
      "source": [
        "To make sure you are running the latest version of TensorFlow,\n",
        "execute the following script in the Google Colab notebook\n",
        "cell. The following script will update your TensorFlow version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7n4ym8oY5ybu",
        "outputId": "9e0700d8-978d-4408-a209-61f317136b3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.67.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC4SQQD27ACe"
      },
      "source": [
        "To check if you are really running TensorFlow version > 2.0,\n",
        "execute the following script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "W-rBE6ko7BND",
        "outputId": "f7249a6d-34db-4e7d-ac4c-c603383f6dd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.18.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9YEuixI3hPS"
      },
      "source": [
        "# **3**\n",
        "# **Python NumPy Library for Data Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JE6ZcE733xMu",
        "outputId": "2411c640-1cf4-4d93-cd67-e18071bf933b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "pip install numpy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3a9P3Es66Uk"
      },
      "source": [
        "Script 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7l74E57y6Shp",
        "outputId": "903e1a50-7da6-42af-9595-80ff90e85ca2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10, 12, 15, 16, 20]\n",
            "[10 12 15 16 20]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "  import numpy as np\n",
        "  nums_list=[10,12,15,16,20]\n",
        "  nums_array=np.array(nums_list)\n",
        "  print(nums_list)\n",
        "  print(nums_array)\n",
        "  type(nums_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukH_oC_L69Zb"
      },
      "source": [
        "Script 2:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEh0NTnX6_Xf",
        "outputId": "a58f0914-6978-4d09-bb59-1e586b1df776"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 3)\n",
            "[[10 12 13]\n",
            " [45 32 16]\n",
            " [45 32 16]]\n"
          ]
        }
      ],
      "source": [
        "row1=[10,12,13]\n",
        "row2=[45,32,16]\n",
        "row3=[45,32,16]\n",
        "\n",
        "nums_2d=np.array([row1,row2,row3])\n",
        "print(nums_2d.shape)\n",
        "print(nums_2d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTt031xL8B8H"
      },
      "source": [
        "Script 3:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9qRycV48DaC",
        "outputId": "4035b2ca-af84-474d-c16b-a8eb0f90770a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 5  6  7  8  9 10]\n"
          ]
        }
      ],
      "source": [
        "nums_arr=np.arange(5,11)\n",
        "print(nums_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USCzFIzD8WxU"
      },
      "source": [
        "Script 4:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9HlFB4X8YN5",
        "outputId": "05b22c85-27aa-476d-9688-005c1873dba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 5  7  9 11]\n"
          ]
        }
      ],
      "source": [
        "nums_arr=np.arange(5,12,2)\n",
        "print(nums_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pHV0oFz8rwj"
      },
      "source": [
        "Script 5:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muGe-D3r8tV5",
        "outputId": "d1b302b7-7316-4e51-8118-08af7cef93b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 1. 1. 1. 1.]\n"
          ]
        }
      ],
      "source": [
        "ones_array=np.ones(6)\n",
        "print(ones_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJm60alW_jO7"
      },
      "source": [
        "Scritp 6:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZZiPEJO_oHj",
        "outputId": "0941c54c-0172-466d-8bdc-1dd43d1a9972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]]\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.float64'>\n"
          ]
        }
      ],
      "source": [
        "ones_array=np.ones((6,4))\n",
        "print(ones_array)\n",
        "print(type(ones_array[0]))\n",
        "print(type(ones_array[0][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS60jrAVCNAE"
      },
      "source": [
        "Script 7:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-TJGv56CPHL",
        "outputId": "92b18e44-7cb3-4118-b909-739de2bbba8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "zeros_array=np.zeros(6)\n",
        "print(zeros_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuIED9AICkTW"
      },
      "source": [
        "Script 8:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IzkFNAPCltj",
        "outputId": "39115003-4b1e-4721-c4bc-3aa355c51b56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "zeros_array=np.zeros((6,4))\n",
        "print(zeros_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MmY7GxMDCY5"
      },
      "source": [
        "Script 9:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2pD7RkmDElM",
        "outputId": "9cc9b6fd-cd9c-4ce2-ef56-2314b0e78b23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "eyes_array=np.eye(5)\n",
        "print(eyes_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv6VnQxVDxNT"
      },
      "source": [
        "Script 10:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvs6cg2RD6js",
        "outputId": "48619ee8-2fa2-4c35-9971-5026b0b70f1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.32175643 0.75663327 0.30528726 0.26301441 0.33828363]\n",
            " [0.58720461 0.79046828 0.05814425 0.8751265  0.40140239]\n",
            " [0.91461773 0.8656336  0.6121277  0.88416695 0.54385623]\n",
            " [0.56578047 0.22521276 0.70624111 0.13202266 0.38920951]]\n"
          ]
        }
      ],
      "source": [
        "uniform_random=np.random.rand(4,5)\n",
        "print(uniform_random)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaqhLFCUHR1o"
      },
      "source": [
        "Script 11:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHptdN5_HTu7",
        "outputId": "27b51c80-6a15-4b57-81e6-60562236283c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.74712341 -0.54637608  1.14326876  0.15145619  0.88577556]\n",
            " [-0.09681656 -0.24031422  0.0423955  -0.62558384 -1.18520116]\n",
            " [-0.15182341  0.33954775 -0.40428057 -0.36423599 -1.16146767]\n",
            " [ 1.26091801  0.21702652 -0.22563687  0.16280577 -0.97963128]]\n"
          ]
        }
      ],
      "source": [
        "normal_random=np.random.randn(4,5)\n",
        "print(normal_random)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNE8gO04IE44"
      },
      "source": [
        "Script 12:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwLvcI0zIGsE",
        "outputId": "d44f1cb3-0b04-4145-8ffc-4b351d1ef039"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11 41 40 24 46]\n"
          ]
        }
      ],
      "source": [
        "integer_random=np.random.randint(10,50,5)\n",
        "print(integer_random)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nauXoB1COvvn"
      },
      "source": [
        "Script 13:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWh9thfVOxyI",
        "outputId": "ae0001a9-6884-4607-e2c0-e57ea51984a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.06185942 0.64443503 0.8502673  0.32685166 0.08804058 0.81227685]\n",
            " [0.72443723 0.68105198 0.57161478 0.06472705 0.65957572 0.38726265]\n",
            " [0.32440893 0.36183747 0.55702871 0.7241518  0.72391913 0.0622471 ]\n",
            " [0.41774518 0.69162014 0.97006265 0.54044499 0.76329782 0.17426024]]\n",
            "[[0.06185942 0.64443503 0.8502673  0.32685166 0.08804058 0.81227685\n",
            "  0.72443723 0.68105198]\n",
            " [0.57161478 0.06472705 0.65957572 0.38726265 0.32440893 0.36183747\n",
            "  0.55702871 0.7241518 ]\n",
            " [0.72391913 0.0622471  0.41774518 0.69162014 0.97006265 0.54044499\n",
            "  0.76329782 0.17426024]]\n"
          ]
        }
      ],
      "source": [
        "uniform_random=np.random.rand(4,6)\n",
        "print(uniform_random)\n",
        "uniform_random=uniform_random.reshape(3,8)\n",
        "print(uniform_random)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PsxQbcPPWIZ"
      },
      "source": [
        "Script 14:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mOWWjiLPXwC",
        "outputId": "e37c4af1-a592-4eb7-c2bb-f9549478a9d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1  2  3  4  5  6  7  8  9 10]\n"
          ]
        }
      ],
      "source": [
        "s=np.arange(1,11)\n",
        "print(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0EemRiHPc2-"
      },
      "source": [
        "Script 15.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84X_dAULPekR",
        "outputId": "e41b7c05-484e-4806-d1de-cb21d5e66fb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "print(s[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2Zna5JzPwqm"
      },
      "source": [
        "Script 16:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUhXpYYBPy0p",
        "outputId": "785c7e65-d79c-4ccb-f5fa-ac9d336826c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 3 4 5 6 7 8 9]\n"
          ]
        }
      ],
      "source": [
        "print(s[1:9])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVnIWKK4QD4J"
      },
      "source": [
        "Script 17:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPvjSZ9nhr4V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vy-hZOS0QGFB",
        "outputId": "3ecae0e2-626a-45a5-9ae6-716fa0f4f29a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 2 3 4 5]\n",
            "[ 6  7  8  9 10]\n"
          ]
        }
      ],
      "source": [
        "print(s[:5])\n",
        "print(s[5:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coj2aPhDQ4ZI"
      },
      "source": [
        "Script 18:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqhG_mPqQ6i8",
        "outputId": "a91d28da-6a55-4693-fcdf-32b3b5708a5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[10 12]\n",
            " [45 32]]\n"
          ]
        }
      ],
      "source": [
        "row1=[10,12,13]\n",
        "row2=[45,32,16]\n",
        "row3=[54,23,61]\n",
        "\n",
        "nums_2d=np.array([row1,row2,row3])\n",
        "print(nums_2d[:2,:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUFdS1haaSu6"
      },
      "source": [
        "Script 21:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcVCOA73aUR6",
        "outputId": "d6ee78b5-e53c-4027-d95b-37d2d2c51f6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2. 3. 4.]\n"
          ]
        }
      ],
      "source": [
        "nums=[4,9,16]\n",
        "np_sqr=np.sqrt(nums)\n",
        "print(np_sqr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvnem9lNauF6"
      },
      "source": [
        "Script 22:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mvOXTzEav_I",
        "outputId": "a806cd6a-7933-4667-a9b8-d8680c7168b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.30258509 2.99573227 3.40119738 3.68887945 3.91202301]\n"
          ]
        }
      ],
      "source": [
        "nums=[10,20,30,40,50]\n",
        "np_log=np.log(nums)\n",
        "print(np_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ao_ZFifnbAz5"
      },
      "source": [
        "Script 23:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FOO4Tds9bCXr",
        "outputId": "56c8d114-c95e-4654-debb-15830c22cfda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.20264658e+04 4.85165195e+08 1.06864746e+13 2.35385267e+17\n",
            " 5.18470553e+21]\n"
          ]
        }
      ],
      "source": [
        "nums=[10,20,30,40,50]\n",
        "n=np.exp(nums)\n",
        "print(n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFaUGhP3gmZU"
      },
      "source": [
        "Script 24:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhdTWDAogn9B",
        "outputId": "48c4acff-ca59-4297-ed72-d960cca40ae2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.54402111  0.91294525 -0.98803162  0.74511316 -0.26237485]\n",
            "[-0.83907153  0.40808206  0.15425145 -0.66693806  0.96496603]\n"
          ]
        }
      ],
      "source": [
        "nums=[10,20,30,40,50]\n",
        "np_sine=np.sin(nums)\n",
        "print(np_sine)\n",
        "\n",
        "np_cos=np.cos(nums)\n",
        "print(np_cos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfYl2heKhP2T"
      },
      "source": [
        "Script 25:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7D0RAJmhRjm",
        "outputId": "551bf946-a81f-4cd9-e221-490d17074c38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.31371409 -3.64023127  1.43760787 -1.43152989]\n",
            " [-1.27240867 -6.69515463 -2.32370999  2.51329999]\n",
            " [ 3.55104323  4.9503389  -0.87373795  3.18530462]\n",
            " [-2.34094143 -3.82009224 -0.5142736   0.53394846]]\n",
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "A=np.random.randn(4,5)\n",
        "B=np.random.randn(5,4)\n",
        "Z=np.dot(A,B)\n",
        "print(Z)\n",
        "\n",
        "C=np.ones((3,3))\n",
        "D=np.zeros((3,3))\n",
        "X=np.dot(C,D)\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVqT6OjRiBxq"
      },
      "source": [
        "Script 26:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPNHAit_iDgj",
        "outputId": "a0d8b86f-f9c7-449a-db33-0eb15cd9834a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1  4  9]\n",
            " [16 25 36]\n",
            " [49 64 81]]\n",
            "[[ 30  36  42]\n",
            " [ 66  81  96]\n",
            " [102 126 150]]\n"
          ]
        }
      ],
      "source": [
        "row1=[1,2,3]\n",
        "row2=[4,5,6]\n",
        "row3=[7,8,9]\n",
        "\n",
        "nums_2d=np.array([row1,row2,row3])\n",
        "multiply=np.multiply(nums_2d,nums_2d)\n",
        "print(multiply)\n",
        "print(np.dot(nums_2d,nums_2d))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1QVsNHqiy2J"
      },
      "source": [
        "Script 27:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kynPej35i0o4",
        "outputId": "be1dec29-caef-4b76-8bff-6613c043d524"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "row1=[1,0,0]\n",
        "row2=[0,1,0]\n",
        "row3=[0,0,1]\n",
        "\n",
        "nums_2d=np.array([row1,row2,row3])\n",
        "inverse=np.linalg.inv(nums_2d)\n",
        "print(inverse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wBUfkgfjQk1"
      },
      "source": [
        "Script 28:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8KvgTKBjSUP",
        "outputId": "2178c2eb-8d52-4255-de11-fee3f83004e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        }
      ],
      "source": [
        "row1=[1,2,3]\n",
        "row2=[4,5,6]\n",
        "row3=[7,8,9]\n",
        "\n",
        "nums_2d=np.array([row1,row2,row3])\n",
        "determinant=np.linalg.det(nums_2d)\n",
        "print(determinant)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WSbmAxfjmdm"
      },
      "source": [
        "Script 29:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPtGj-SxjoDK",
        "outputId": "15cd1f85-9ab5-4bc8-d95b-2bfad2536dbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n"
          ]
        }
      ],
      "source": [
        "row1=[1,2,3]\n",
        "row2=[4,5,6]\n",
        "row3=[7,8,9]\n",
        "\n",
        "nums_2d=np.array([row1,row2,row3])\n",
        "trace=np.trace(nums_2d)\n",
        "print(trace)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR25lpl6j1T7"
      },
      "source": [
        "Exercise 3.1\n",
        "\n",
        "Question 1:\n",
        "Which NumPy function is used for the element-wise multiplication of two matrices?\n",
        "\n",
        "A. np.dot(matrix1, matrix2)\n",
        "\n",
        "**B. np.multiply(matrix1, matrix2)**\n",
        "\n",
        "C. np.elementwise(matrix1, matrix2)\n",
        "\n",
        "D. None of the above\n",
        "\n",
        "\n",
        "\n",
        "Question 2:\n",
        "To generate an identity matrix of four rows and four columns, which of the following functions can be used?\n",
        "\n",
        "A. np.identity(4,4)\n",
        "\n",
        "B. np.id(4,4)\n",
        "\n",
        "**C. np.eye(4,4)**\n",
        "\n",
        "D. All of the above\n",
        "\n",
        "\n",
        "\n",
        "Question 3:\n",
        "How to create the array of numbers 4,7,10,13,16 with NumPy:\n",
        "\n",
        "A. np.arange(3, 16, 3)\n",
        "\n",
        "B. np.arange(4, 16, 3)\n",
        "\n",
        "C. np.arange(4, 15,3)\n",
        "\n",
        "**D. None of the above**\n",
        "\n",
        "\n",
        "\n",
        "Exercise 3.2\n",
        "\n",
        "Create a random NumPy array of five rows and four columns. Using array indexing and slicing, display the items from row three to end and column two to end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GR4KkzJJlMxA",
        "outputId": "49e40de3-0cb3-4352-cba7-f2fe4fe28143"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.44864613 0.13940789 0.97045414 0.29399015]\n",
            " [0.55284755 0.00490626 0.25550572 0.13366116]\n",
            " [0.21514879 0.12200679 0.36650781 0.8296915 ]\n",
            " [0.05215155 0.9605018  0.80516314 0.24422996]\n",
            " [0.68127622 0.06764503 0.3799992  0.22165234]]\n",
            "[[0.12200679 0.36650781 0.8296915 ]\n",
            " [0.9605018  0.80516314 0.24422996]\n",
            " [0.06764503 0.3799992  0.22165234]]\n"
          ]
        }
      ],
      "source": [
        "a=np.random.rand(5,4)\n",
        "print(a)\n",
        "print(a[2:,1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCp8ojXmmuGl",
        "outputId": "221e9121-c32e-4bf5-944e-0fcd41428d7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.34683874 0.29961746 0.98921253 0.30279029 0.18983975]\n",
            " [0.78601871 0.15107351 0.53278371 0.74663633 0.3896812 ]\n",
            " [0.13240156 0.57167281 0.20694586 0.64905593 0.60139796]\n",
            " [0.76984861 0.77714967 0.68447657 0.23167174 0.57294626]]\n",
            "Result\n",
            "[[0.64905593 0.60139796]\n",
            " [0.23167174 0.57294626]]\n"
          ]
        }
      ],
      "source": [
        "uniform_random = np.random.rand(4, 5)\n",
        "print(uniform_random)\n",
        "print(\"Result\")\n",
        "print(uniform_random[2:,3:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDK4gBGK3-bg"
      },
      "source": [
        "# **4**\n",
        "# **Introduction to Pandas Library for Data Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGhTfmnl4Djh",
        "outputId": "2e0d90f0-9b05-461e-b0e5-313ffcfb0d2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxGBdeYe4KxC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItN9wsYr4vaA"
      },
      "source": [
        "Script 1:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "gAQakx8N4w_o",
        "outputId": "b63d629a-7cd6-49ce-941e-cad6afc9c431"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'titanic_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-d537371cb74a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtitanic_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"titanic_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtitanic_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'titanic_data.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "titanic_data=pd.read_csv(\"titanic_data.csv\")\n",
        "titanic_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DxOoeA85GOs"
      },
      "source": [
        "Script 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w56OMscD5l5f"
      },
      "outputs": [],
      "source": [
        "titanic_pclass1=titanic_data.Pclass==1\n",
        "print(titanic_pclass1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTl8Q8L16DdS"
      },
      "source": [
        "Script 3:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vd94IDui6FsA"
      },
      "outputs": [],
      "source": [
        "titanic_pclass1=titanic_data.Pclass==1\n",
        "titanic_pclass1_data=titanic_data[titanic_pclass1]\n",
        "titanic_pclass1_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwpjFF8z6kYu"
      },
      "source": [
        "Script 4:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CA0lMxo6mG6"
      },
      "outputs": [],
      "source": [
        "titanic_pclass1_data=titanic_data[titanic_data.Pclass==1]\n",
        "titanic_pclass1_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-u0TTEp7JIh"
      },
      "source": [
        "Script 5:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Scdf5VZB7LBP"
      },
      "outputs": [],
      "source": [
        "ages=[20,21,22]\n",
        "age_dataset=titanic_data[titanic_data[\"Age\"].isin(ages)]\n",
        "age_dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6dGsmoaHoeR"
      },
      "source": [
        "Script 6:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fe1B6CScHqPh"
      },
      "outputs": [],
      "source": [
        "ages=[20,21,22]\n",
        "ageclass_dataset=titanic_data[titanic_data[\"Age\"].isin(ages)&(titanic_data[\"Pclass\"]==1)]\n",
        "ageclass_dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4xSEXqmJGhc"
      },
      "source": [
        "Script 7:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T2_YD1rJI7o"
      },
      "outputs": [],
      "source": [
        "titanic_data_filter=titanic_data.filter([\"Name\", \"Sex\",\"Age\"])\n",
        "titanic_data_filter.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxP14C6RJ8sd"
      },
      "source": [
        "Script 8:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "oW7HVDJTJ-Ji"
      },
      "outputs": [],
      "source": [
        "titanic_data_filter = titanic_data.drop([\"Name\", \"Sex\",\"Age\"],axis=1)\n",
        "titanic_data_filter.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iueMgCh0dKE1"
      },
      "source": [
        "Script 9:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSvQyuFCdNjj"
      },
      "outputs": [],
      "source": [
        "titanic_pclass1_data=titanic_data[titanic_data.Pclass==1]\n",
        "print(titanic_pclass1_data.shape)\n",
        "\n",
        "titanic_pclass2_data=titanic_data[titanic_data.Pclass==2]\n",
        "print(titanic_pclass2_data.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLJs2ZaleP2c"
      },
      "source": [
        "Script 10:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAVqhSpNeRc0",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "final_data=titanic_pclass1_data.append(titanic_pclass2_data, ignore_index=True)\n",
        "print(final_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDhD0wdQfEMF"
      },
      "source": [
        "Script 11:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXOEGzA4fFhs"
      },
      "outputs": [],
      "source": [
        "final_data=pd.concat([titanic_pclass1_data,titanic_pclass2_data])\n",
        "print(final_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP6AOfmqfxiI"
      },
      "source": [
        "Script 12:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "n9KeQJbEfzGM"
      },
      "outputs": [],
      "source": [
        "df1=final_data[:200]\n",
        "print('---DF1---')\n",
        "print(df1.shape)\n",
        "print(df1.head())\n",
        "print('------')\n",
        "\n",
        "df2=final_data[200:]\n",
        "print('---DF2---')\n",
        "print(df2.shape)\n",
        "print(df2.head())\n",
        "print('------')\n",
        "\n",
        "final_data2=pd.concat([df1,df2], axis=1, ignore_index=True)\n",
        "print('---FINAL DATA---')\n",
        "print(final_data2.shape)\n",
        "print(final_data2.head())\n",
        "print('------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLoBIB40h4zs"
      },
      "source": [
        "Script 13:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OevkLrkh_Fl"
      },
      "outputs": [],
      "source": [
        "age_sorted_data=titanic_data.sort_values(by=[\"Age\"])\n",
        "age_sorted_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HCNVZSWiWnl"
      },
      "source": [
        "Script 14:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKZ-cMEoiX2M"
      },
      "outputs": [],
      "source": [
        "age_sorted_data=titanic_data.sort_values(by=[\"Age\"],ascending=False)\n",
        "age_sorted_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUp4xGT_ik1I"
      },
      "source": [
        "Script 15:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "go4Hm9fyimMT"
      },
      "outputs": [],
      "source": [
        "age_sorted_data=titanic_data.sort_values(by=[\"Age\",\"Fare\"], ascending=False)\n",
        "age_sorted_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tiurZfcUZ75"
      },
      "source": [
        "Script 16:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhsvyKjfUbn5"
      },
      "outputs": [],
      "source": [
        "updated_class=titanic_data.Pclass.apply(lambda x:x+2)\n",
        "updated_class.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ChOEYUPVHx0"
      },
      "source": [
        "Script 17:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Et9ZJTfoVJqY"
      },
      "outputs": [],
      "source": [
        "def mult(x):\n",
        "  return x*2\n",
        "\n",
        "updated_class=titanic_data.Pclass.apply(mult)\n",
        "updated_class.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsqcDfRbVymE"
      },
      "source": [
        "Script 18:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0apDwuL2V01x"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "flights_data=sns.load_dataset('flights')\n",
        "flights_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cTo9UCVWUp5"
      },
      "source": [
        "Script 19:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-6V_S-kWWVX"
      },
      "outputs": [],
      "source": [
        "flights_data_pivot=flights_data.pivot_table(index='month',columns='year',values='passengers')\n",
        "flights_data_pivot.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ1i2XuTXAzy"
      },
      "source": [
        "Script 20:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9YHbeoSXCjx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "titanic_data=pd.read_csv('titanic_data.csv')\n",
        "titanic_data.head()\n",
        "pd.crosstab(titanic_data.Pclass, titanic_data.Age, margins=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeiXa2lEXzit"
      },
      "source": [
        "Script 21:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9oCEJryX1Ny"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "titanic_data.head()\n",
        "titanic_data.Fare=np.where(titanic_data.Age>20, titanic_data.Fare+5, titanic_data.Fare)\n",
        "titanic_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYEE0MldYmOz"
      },
      "source": [
        "**Exercise 4.1**\n",
        "\n",
        "Question 1\n",
        "\n",
        "In order to horizontally concatenate two Pandas dataframes, the value for the axis attribute should be set to:\n",
        "\n",
        "A. 0\n",
        "\n",
        "**B. 1**\n",
        "\n",
        "C. 2\n",
        "\n",
        "D. None of the above\n",
        "\n",
        "\n",
        "Question 2\n",
        "\n",
        "Which function is used to sort a Pandas dataframe by column value?\n",
        "\n",
        "A. sort_dataframe()\n",
        "\n",
        "B. sort_rows()\n",
        "\n",
        "**C. sort_values()**\n",
        "\n",
        "D. sort_records()\n",
        "\n",
        "\n",
        "Question 3\n",
        "\n",
        "To filter columns from a Pandas dataframe, you have to pass a list of column names to one of the following methods:\n",
        "\n",
        "**A. filter()**\n",
        "\n",
        "B. filter_columns()\n",
        "\n",
        "C. apply_filter()\n",
        "\n",
        "D. None of the above()\n",
        "\n",
        "\n",
        "Exercise 4.2\n",
        "\n",
        "Use the apply function to subtract 10 from the Fare column of the Titanic dataset, without using a lambda expression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgvFoFYQZpzL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "titanic_data=pd.read_csv('titanic_data.csv')\n",
        "\n",
        "def sub(x):\n",
        "  return x-10\n",
        "\n",
        "new_titanic_data=titanic_data.Fare.apply(sub)\n",
        "print(new_titanic_data.head())\n",
        "print('---')\n",
        "print(titanic_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qZF2nTp9BlF"
      },
      "source": [
        "# **5**\n",
        "# **Data Visualization via Matplotlib, Seaborn, and Pandas Libraries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3pRV6ux9FR-"
      },
      "source": [
        "5.2. Data Visualization via Matplotlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa0xSKnE9N1q"
      },
      "source": [
        "Script 1:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaUJ85pb9Gtv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "x_vals=np.linspace(0,20,20)\n",
        "y_vals=[math.sqrt(i) for i in x_vals]\n",
        "plt.plot(x_vals,y_vals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_gFyCJHClsF"
      },
      "source": [
        "Script 2:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmhZVWOYCnL1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "x_vals=np.linspace(0,20,20)\n",
        "y_vals=[math.sqrt(i) for i in x_vals]\n",
        "\n",
        "fig=plt.figure()\n",
        "ax=plt.axes()\n",
        "ax.plot(x_vals,y_vals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eI-ca6MD9vt"
      },
      "source": [
        "Script 3:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xvh6SAVD__7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"]=[8,6]\n",
        "\n",
        "x_vals=np.linspace(0,20,20)\n",
        "y_vals=[math.sqrt(i) for i in x_vals]\n",
        "plt.plot(x_vals,y_vals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dqt174bFK-y"
      },
      "source": [
        "Script 4:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_WLxDjyFNYH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "x_vals=np.linspace(0,20,20)\n",
        "y_vals=[math.sqrt(i) for i in x_vals]\n",
        "\n",
        "plt.xlabel('X Values')\n",
        "plt.ylabel('Y Values')\n",
        "plt.title('Square Roots')\n",
        "\n",
        "plt.plot(x_vals,y_vals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atONt-VCGC6C"
      },
      "source": [
        "Script 5:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAGXi7OVGFI1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "x_vals=np.linspace(0,20,20)\n",
        "y_vals=[math.sqrt(i) for i in x_vals]\n",
        "\n",
        "plt.xlabel('X Values')\n",
        "plt.ylabel('Y Values')\n",
        "plt.title('Square Roots')\n",
        "\n",
        "plt.plot(x_vals,y_vals,'r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYh_E8RPG5Dv"
      },
      "source": [
        "Script 6:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wZpAYMCG6zm"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "x_vals=np.linspace(0,20,20)\n",
        "y_vals=[math.sqrt(i) for i in x_vals]\n",
        "\n",
        "plt.xlabel('X Values')\n",
        "plt.ylabel('Y Values')\n",
        "plt.title('Square Roots')\n",
        "plt.plot(x_vals,y_vals,'r',label='Square Root')\n",
        "plt.legend(loc='upper center')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eFLwWoWMYLL"
      },
      "source": [
        "Script 7:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gm0yA-DKMZ02"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "x_vals=np.linspace(0,20,20)\n",
        "y_vals=[math.sqrt(i)for i in x_vals]\n",
        "y2_vals=x_vals**3\n",
        "\n",
        "plt.xlabel('X Values')\n",
        "plt.ylabel('Y Values')\n",
        "plt.title('Square Roots')\n",
        "plt.plot(x_vals,y_vals,'r',label='Square Roots')\n",
        "plt.plot(x_vals,y2_vals,'b', label='Cube')\n",
        "plt.legend(loc='upper center')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUtwTvT0QFmS"
      },
      "source": [
        "Script 8:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jU9gL2NRQHvu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data=pd.read_csv('iris_data.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-JHklshQSVp"
      },
      "source": [
        "Script 9:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7g9PBOaQUfc"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9yL33A_QuCu"
      },
      "source": [
        "Script 10:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LxOhYTiQv3G"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "plt.xlabel('Sepal Length')\n",
        "plt.ylabel('Petal Length')\n",
        "plt.title('Sepal vs Petal Length')\n",
        "plt.plot(data['sepal_length'],data['petal_length'],'b')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9dUlUBoY_ro"
      },
      "source": [
        "Script 11:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBkBKav_ZCLq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "plt.xlabel('Sepal Length')\n",
        "plt.ylabel('Petal Length')\n",
        "plt.title('Sepal vs Petal Length')\n",
        "plt.scatter(data['sepal_length'],data['petal_length'],c='b')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6mePQqSc4Hg"
      },
      "source": [
        "Script 12:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqC1I6z-c6g5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data=pd.read_csv('titanic_data.csv')\n",
        "data.head()\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g132GbSRdJvZ"
      },
      "source": [
        "Script 13:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNVLDWpZdMBU"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "plt.xlabel('Gender')\n",
        "plt.ylabel('Ages')\n",
        "plt.title('Gender vs Ages')\n",
        "plt.bar(data['Sex'],data['Age'])\n",
        "print(data['Age'].max())\n",
        "print(data['Age'].max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fSpZFiBg2MM"
      },
      "source": [
        "Script 16:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfh-Gahdg7iG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "plt.title('Age Histogram')\n",
        "plt.hist(data['Age'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOQpvyDXh-NH"
      },
      "source": [
        "Script 17:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TAKXXAJh_sW"
      },
      "outputs": [],
      "source": [
        "labels='IT', 'Marketing','Data Science', 'Finance'\n",
        "values=[500,156,300,510]\n",
        "explode=(0.05,0.05,0.05,0.1)\n",
        "\n",
        "plt.pie(values, explode=explode, labels=labels, autopct='1%.1f%%', shadow=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJMtjI0cjFAH"
      },
      "outputs": [],
      "source": [
        "pip install seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biYBCNKoOMU1"
      },
      "source": [
        "Script 18:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXKSFhIhj4o0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.rcParams['figure.figsize']=[10,8]\n",
        "\n",
        "tips_data=sns.load_dataset('tips')\n",
        "\n",
        "tips_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX3KSJvqOOPp"
      },
      "source": [
        "Script 19:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCgMA-2uOToc"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.rcParams['figure.figsize']=[10,8]\n",
        "sns.distplot(tips_data['total_bill'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lHpU5vEO6ar"
      },
      "source": [
        "Script 20:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFBd23JJO773"
      },
      "outputs": [],
      "source": [
        "sns.jointplot(x='total_bill',y='tip',data=tips_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RERSPxkjPfL6"
      },
      "source": [
        "Script 21:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxJvqd2mPg-H"
      },
      "outputs": [],
      "source": [
        "sns.jointplot(x='size',y='total_bill',data=tips_data,kind='reg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFM3RVzQRYHw"
      },
      "source": [
        "Script 22:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3M25gIqPRaPO"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(data=tips_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZuQ0eRUR5Hr"
      },
      "source": [
        "Script 23:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uU9Rt8EPR6ZZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.rcParams['figure.figsize']=[8,6]\n",
        "sns.set_style('darkgrid')\n",
        "\n",
        "titanic_data=sns.load_dataset('titanic')\n",
        "\n",
        "titanic_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZiosZJrSr8Z"
      },
      "source": [
        "Script 24:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aCNirQzStWg"
      },
      "outputs": [],
      "source": [
        "sns.barplot(x='pclass',y='age', data=titanic_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUW4w88STN9Y"
      },
      "source": [
        "Script 25:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wpp4fTvaTPUf"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x='pclass',data=titanic_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ER8jmf_OTiGb"
      },
      "source": [
        "Script 26:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2I4dVPPgTjmA"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(x=titanic_data['age'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdU5LqkBV9Qz"
      },
      "source": [
        "Script 27:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a11ISx8V-0C"
      },
      "outputs": [],
      "source": [
        "sns.violinplot(x='alone',y='age',data=titanic_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EYs0M4-YTqp"
      },
      "source": [
        "Script 28:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jF3CQFCDYVoY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "titanic_data=pd.read_csv('titanic_data.csv')\n",
        "titanic_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1BISENLY3gD"
      },
      "source": [
        "Script 29:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jF1b3sVBY4s-"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "titanic_data['Age'].hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8CJ4lyaZU4K"
      },
      "source": [
        "Script 30:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gKIcdNPZWlq"
      },
      "outputs": [],
      "source": [
        "flights_data=sns.load_dataset('flights')\n",
        "flights_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-rG1IyfZsxB"
      },
      "source": [
        "Script 31:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3--UOTNUZuDP"
      },
      "outputs": [],
      "source": [
        "flights_data.plot.line(y='passengers',figsize=(8,6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPYySSMBaK84"
      },
      "source": [
        "Script 32:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrlY4SESaMqI"
      },
      "outputs": [],
      "source": [
        "flights_data.plot.scatter(x='year',y='passengers',figsize=(8,6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DyYqlSYbJW7"
      },
      "source": [
        "Script 33:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_18z9mhbKyY"
      },
      "outputs": [],
      "source": [
        "sex_mean=titanic_data.groupby('Sex')['Age'].mean()\n",
        "print(sex_mean)\n",
        "print(type(sex_mean.tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xB2e4k_btO-"
      },
      "source": [
        "Script 34:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBEfFLH5bu1h"
      },
      "outputs": [],
      "source": [
        "df=pd.DataFrame({'Gender':['Famale','Male'],'Age':sex_mean.tolist()})\n",
        "ax=df.plot.bar(x='Gender',y='Age',figsize=(8,6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYYkHGD-cavp"
      },
      "source": [
        "**Exercise 5.1**\n",
        "\n",
        "**Question 1**\n",
        "\n",
        "Which Pandas function is used to plot a horizontal bar plot:\n",
        "\n",
        "A. horz_bar()\n",
        "\n",
        "**B. barh()**\n",
        "\n",
        "C. bar_horizontal()\n",
        "\n",
        "D. horizontal_bar()\n",
        "\n",
        "\n",
        "\n",
        "**Question 2:**\n",
        "\n",
        "To create a legend, the value for which of the following parameters is needed to be specified?\n",
        "\n",
        "A. title\n",
        "\n",
        "**B. label**\n",
        "\n",
        "C. axis\n",
        "\n",
        "D. All of the above\n",
        "\n",
        "\n",
        "\n",
        "**Question 3:**\n",
        "\n",
        "How to show percentage values on a Matplotlib Pie Chart?\n",
        "\n",
        "**A. autopct = %1.1f%%**\n",
        "\n",
        "B. percentage = %1.1f%%\n",
        "\n",
        "C. perc = %1.1f%%\n",
        "\n",
        "D. None of the Above\n",
        "\n",
        "\n",
        "\n",
        "**Exercise 5.2**\n",
        "\n",
        "Plot two scatter plots on the same graph using the tips_dataset. In the first scatter plot, display values from the total_bill column on the x-axis and from the tip column on the y-axis. The color of the first scatter plot should be green.\n",
        "\n",
        "In the second scatter plot, display values from the total_bill column on the x-axis and from the size column on the y-axis. The color of the second scatter plot should be blue, and markers should be x."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7Tm_MD4dZ8h"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "tips_data=sns.load_dataset('tips')\n",
        "sns.scatterplot(x=\"total_bill\", y=\"tip\", data=tips_data, color = 'g')\n",
        "sns.scatterplot(x=\"total_bill\", y=\"size\", data=tips_data, color = 'b', marker = 'x')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uTT_Q5OLbBG"
      },
      "source": [
        "# **6**\n",
        "\n",
        "# **Solving Regression Problems in Machine Learning Using Sklearn Librar**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hADE6hIGLdCO"
      },
      "source": [
        "Script 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BRH0cHyMLj8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "sns.get_dataset_names()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fiPStyqMZ4U"
      },
      "source": [
        "Script 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HFOsJQJgMbFs"
      },
      "outputs": [],
      "source": [
        "tips_df=sns.load_dataset('tips')\n",
        "tips_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tltanDKWMqxf"
      },
      "source": [
        "Script 3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Vo_cR5EDMsBc"
      },
      "outputs": [],
      "source": [
        "diamond_df=sns.load_dataset('diamonds')\n",
        "diamond_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wiyt9UDbNZdF"
      },
      "source": [
        "Script 4:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiCDr0lrNaw6"
      },
      "outputs": [],
      "source": [
        "x=tips_df.drop(['tip'],axis=1)\n",
        "y=tips_df['tip']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS6BiYnbNqqx"
      },
      "source": [
        "Script 5:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YUJJFmfNswy"
      },
      "outputs": [],
      "source": [
        "x.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHuhbLY3N4Y1"
      },
      "source": [
        "Script 6:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOw8LZZDN6Gs"
      },
      "outputs": [],
      "source": [
        "y.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1gy2tSrOKM2"
      },
      "source": [
        "Script 7:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpaPynUCOLlT"
      },
      "outputs": [],
      "source": [
        "numerical=x.drop(['sex','smoker','day','time'],axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km2Fsg-JOZPU"
      },
      "source": [
        "Script 8:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmAxt4A7OaeO"
      },
      "outputs": [],
      "source": [
        "numerical.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HClCErq4Oha4"
      },
      "source": [
        "Script 9:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Pv4r-nDOirs"
      },
      "outputs": [],
      "source": [
        "categorical=x.filter(['sex','smoker','day','time'])\n",
        "categorical.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcD3UVs-POJb"
      },
      "source": [
        "Script 10:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pl2pvhGPPXY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "cat_numerical=pd.get_dummies(categorical, drop_first=True,dtype=int)\n",
        "cat_numerical.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHmIDlQoQjWA"
      },
      "source": [
        "Script 11:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x054XAROQlL0"
      },
      "outputs": [],
      "source": [
        "x=pd.concat([numerical,cat_numerical],axis=1)\n",
        "x.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-Z0i1OIeOtq"
      },
      "source": [
        "Script 12:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDDbSQEKeQGI"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=0)\n",
        "x_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBTy21lee5m2"
      },
      "source": [
        "Script 13:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xDzavvRne6zS"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc=StandardScaler()\n",
        "#scaling the training set\n",
        "x_train=sc.fit_transform(x_train)\n",
        "#scaling the test set\n",
        "x_test=sc.transform(x_test)\n",
        "x_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgkOC5kvgo2q"
      },
      "source": [
        "Script 14:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1p-qCvbgqBT"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "#traning the algorithm\n",
        "lin_reg=LinearRegression()\n",
        "regressor=lin_reg.fit(x_train, y_train)\n",
        "\n",
        "#making predictionson test set\n",
        "y_pred=regressor.predict(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lq32k-khkEc"
      },
      "source": [
        "Script 15:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axr_wfSxhlmT"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "print('Mean Absolute Error: ',metrics.mean_absolute_error(y_test,y_pred))\n",
        "print('Mean Squared Error: ', metrics.mean_squared_error(y_test,y_pred))\n",
        "print('Root Mean Squared Error: ', np.sqrt(metrics.mean_squared_error(y_test,y_pred)))\n",
        "\n",
        "plt.plot(x_test, y_test, color='red')\n",
        "\n",
        "plt.plot(x_test, y_pred, color='blue')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F85sdjJljzW0"
      },
      "source": [
        "EXAMPLE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "842tLi21jtXD"
      },
      "outputs": [],
      "source": [
        "# Code source: Jaques Grobler\n",
        "# License: BSD 3 clause\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load the diabetes dataset\n",
        "diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n",
        "\n",
        "# Use only one feature\n",
        "diabetes_X = diabetes_X[:, np.newaxis, 2]\n",
        "\n",
        "# Split the data into training/testing sets\n",
        "diabetes_X_train = diabetes_X[:-20]\n",
        "diabetes_X_test = diabetes_X[-20:]\n",
        "\n",
        "# Split the targets into training/testing sets\n",
        "diabetes_y_train = diabetes_y[:-20]\n",
        "diabetes_y_test = diabetes_y[-20:]\n",
        "\n",
        "# Create linear regression object\n",
        "regr = linear_model.LinearRegression()\n",
        "\n",
        "# Train the model using the training sets\n",
        "regr.fit(diabetes_X_train, diabetes_y_train)\n",
        "\n",
        "# Make predictions using the testing set\n",
        "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
        "\n",
        "# The coefficients\n",
        "print(\"Coefficients: \\n\", regr.coef_)\n",
        "# The mean squared error\n",
        "print(\"Mean squared error: %.2f\" % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
        "# The coefficient of determination: 1 is perfect prediction\n",
        "print(\"Coefficient of determination: %.2f\" % r2_score(diabetes_y_test, diabetes_y_pred))\n",
        "\n",
        "# Plot outputs\n",
        "plt.scatter(diabetes_X_test, diabetes_y_test, color=\"black\")\n",
        "plt.plot(diabetes_X_test, diabetes_y_pred, color=\"blue\", linewidth=3)\n",
        "\n",
        "plt.xticks(())\n",
        "plt.yticks(())\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3ZdG6YxlT7N"
      },
      "source": [
        "Script 16:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOjRm9gJlXTP"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "knn_reg=KNeighborsRegressor(n_neighbors=5)\n",
        "regressor=knn_reg.fit(x_train,y_train)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_pred=regressor.predict(x_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "print('Mean Absolute Error: ',metrics.mean_absolute_error(y_test,y_pred))\n",
        "print('Mean Squared Error: ', metrics.mean_squared_error(y_test,y_pred))\n",
        "print('Root Mean Squared Error: ', np.sqrt(metrics.mean_squared_error(y_test,y_pred)))\n",
        "\n",
        "plt.plot(x_test, y_test, color='red')\n",
        "\n",
        "plt.plot(x_test, y_pred, color='blue')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErEAfN7BMYOn"
      },
      "source": [
        "Script 17:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pM7LStoMZoX"
      },
      "outputs": [],
      "source": [
        "#training and testing the random forest\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rf_reg=RandomForestRegressor(random_state=42,n_estimators=500)\n",
        "regressor=rf_reg.fit(x_train,y_train)\n",
        "y_pred=regressor.predict(x_test)\n",
        "\n",
        "#evaluating algorithm performance\n",
        "from sklearn import metrics\n",
        "print('Mean Absolute Error: ',metrics.mean_absolute_error(y_test,y_pred))\n",
        "print('Mean Squared Error: ', metrics.mean_squared_error(y_test,y_pred))\n",
        "print('Root Mean Squared Error: ', np.sqrt(metrics.mean_squared_error(y_test,y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmxZfarEDIfA"
      },
      "source": [
        "Script 18:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTvxbBmPDJ9F"
      },
      "outputs": [],
      "source": [
        "#training and testing the SVM\n",
        "\n",
        "from sklearn import svm\n",
        "svm_reg=svm.SVR()\n",
        "\n",
        "regressor=svm_reg.fit(x_train,y_train)\n",
        "y_pred=regressor.predict(x_test)\n",
        "\n",
        "#evaluating algorithm performance\n",
        "from sklearn import metrics\n",
        "print('Mean Absolute Error: ',metrics.mean_absolute_error(y_test,y_pred))\n",
        "print('Mean Squared Error: ', metrics.mean_squared_error(y_test,y_pred))\n",
        "print('Root Mean Squared Error: ', np.sqrt(metrics.mean_squared_error(y_test,y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "debM2sTIEecy"
      },
      "source": [
        "Script 19:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmxAf_YEEgGF"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "print(cross_val_score(regressor,x,y,cv=5,scoring='neg_mean_absolute_error'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpGhG7OYFBwb"
      },
      "source": [
        "Script 20:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4usm5ZzFDKy"
      },
      "outputs": [],
      "source": [
        "tips_df.loc[100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbish-6fFQQP"
      },
      "source": [
        "Script 21:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYi6ipI9FSIA"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rf_reg=RandomForestRegressor(random_state=42,n_estimators=500)\n",
        "regressor=rf_reg.fit(x_train,y_train)\n",
        "single_record=sc.transform(x.values[100].reshape(1,-1))\n",
        "predicted_tip=regressor.predict(single_record)\n",
        "print(predicted_tip)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsUQr3-yGagA"
      },
      "source": [
        "**Exercise 6.1**\n",
        "\n",
        "**Question 1**\n",
        "\n",
        "Among the following, which one is an example of a regression output?\n",
        "\n",
        "A. True\n",
        "\n",
        "B. Red\n",
        "\n",
        "**C. 2.5**\n",
        "\n",
        "D. None of the above\n",
        "\n",
        "\n",
        "\n",
        "**Question 2**\n",
        "\n",
        "Which of the following algorithm is a lazy algorithm?\n",
        "\n",
        "A. Random Forest\n",
        "\n",
        "**B. KNN**\n",
        "\n",
        "C. SVM\n",
        "\n",
        "D. Linear Regression\n",
        "\n",
        "\n",
        "\n",
        "**Question 3**\n",
        "Which of the following algorithm is not a regression metric?\n",
        "\n",
        "A. Accuracy\n",
        "\n",
        "B. Recall\n",
        "\n",
        "C. F1 Measure\n",
        "\n",
        "**D. All of the above**\n",
        "\n",
        "\n",
        "**Exercise 6.2**\n",
        "\n",
        "Using the Diamonds dataset from the Seaborn library, train a regression algorithm of your choice, which predicts the price of the diamond. Perform all the preprocessing steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6i7amxyHjZR"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import svm\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "\n",
        "#step 1: crear data frame de datos\n",
        "diamonds_df=sns.load_dataset('diamonds')\n",
        "\n",
        "\n",
        "#step 2: generar las listas del data frame x e y\n",
        "x=diamonds_df.drop(['price'],axis=1)\n",
        "y=diamonds_df['price']\n",
        "\n",
        "#step 3: genear el data frame de numeros\n",
        "  #step 3.1: separar el texto de numeros\n",
        "numerical=x.drop(['cut','color','clarity'],axis=1)\n",
        "categorical=x.filter(['cut','color','clarity'])\n",
        "\n",
        "  #step 3.2: convertir el texto a numeros que se puedan manejar\n",
        "cat_numerical=pd.get_dummies(categorical, drop_first=True,dtype=int)\n",
        "\n",
        "  #step 3.3: unir los 2 dataframe de solo numeros\n",
        "x=pd.concat([numerical,cat_numerical],axis=1)\n",
        "\n",
        "\n",
        "#step 4: generar los datos para entrenamiento y testeo\n",
        "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=0)\n",
        "\n",
        "#step 5: normalizar los valores\n",
        "sc=StandardScaler()\n",
        "  #scaling the training set\n",
        "x_train=sc.fit_transform(x_train)\n",
        "  #scaling the test set\n",
        "x_test=sc.transform(x_test)\n",
        "\n",
        "\n",
        "#step 6: aplicar los algoritmos\n",
        "\n",
        "#--LinearRegression--\n",
        "#traning the algorithm\n",
        "lin_reg=LinearRegression()\n",
        "regressor=lin_reg.fit(x_train, y_train)\n",
        "#making predictionson test set\n",
        "y_pred_ln=regressor.predict(x_test)\n",
        "\n",
        "\n",
        "#--KNeighborsRegressor--\n",
        "#traning the algorithm\n",
        "knn_reg=KNeighborsRegressor(n_neighbors=5)\n",
        "regressor=knn_reg.fit(x_train,y_train)\n",
        "#making predictionson test set\n",
        "y_pred_kn=regressor.predict(x_test)\n",
        "\n",
        "\n",
        "#--RandomForestRegressor--\n",
        "#traning the algorithm\n",
        "rf_reg=RandomForestRegressor(random_state=42,n_estimators=500)\n",
        "regressor=rf_reg.fit(x_train,y_train)\n",
        "#making predictionson test set\n",
        "y_pred_rf=regressor.predict(x_test)\n",
        "\n",
        "\n",
        "#--SVM--\n",
        "#traning the algorithm\n",
        "svm_reg=svm.SVR()\n",
        "regressor=svm_reg.fit(x_train,y_train)\n",
        "#making predictionson test set\n",
        "y_pred_svm=regressor.predict(x_test)\n",
        "\n",
        "\n",
        "y_pred=[y_pred_ln,y_pred_kn,y_pred_rf,y_pred_svm]\n",
        "algoritmo=['--LinearRegression--','--KNeighborsRegressor--','--RandomForestRegressor--','--SVM--']\n",
        "for x in range(0,4):\n",
        "  print(algoritmo[x])\n",
        "  print('\\tMean Absolute Error: ',metrics.mean_absolute_error(y_test,y_pred[x]))\n",
        "  print('\\tMean Squared Error: ', metrics.mean_squared_error(y_test,y_pred[x]))\n",
        "  print('\\tRoot Mean Squared Error: ', np.sqrt(metrics.mean_squared_error(y_test,y_pred[x])))\n",
        "  print('\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFqzSHNwjfFZ"
      },
      "source": [
        "RESPUESTA DEL LIBRO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZzvIP01iYU4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "diamonds_df = sns.load_dataset(\"diamonds\")\n",
        "\n",
        "X = diamonds_df.drop(['price'], axis=1)\n",
        "\n",
        "y = diamonds_df[\"price\"]\n",
        "\n",
        "numerical = X.drop(['cut', 'color', 'clarity'], axis = 1)\n",
        "\n",
        "categorical = X.filter(['cut', 'color', 'clarity'])\n",
        "\n",
        "cat_numerical = pd.get_dummies(categorical,drop_first=True)\n",
        "\n",
        "X = pd.concat([numerical, cat_numerical], axis = 1)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform (X_test)\n",
        "\n",
        "from sklearn import svm\n",
        "svm_reg = svm.SVR()\n",
        "regressor = svm_reg.fit(X_train, y_train)\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
        "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
        "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrXhmXA1rozc"
      },
      "source": [
        "# **7**\n",
        "\n",
        "# **Solving Classification Problems in Machine Learning Using Sklearn Library**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5I2m1zCrquQ"
      },
      "source": [
        "Script 1:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UkZMAxTrzpw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwmYMxmrsGBx"
      },
      "source": [
        "Script 2:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtrxdofdsHPQ"
      },
      "outputs": [],
      "source": [
        "# Se crea el DataFrame de los datos\n",
        "churn_df=pd.read_csv('customer_churn.csv')\n",
        "churn_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcUtCVmSsi3M"
      },
      "source": [
        "Script 3:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYFrrXylskxX"
      },
      "outputs": [],
      "source": [
        "#Se elimina las columnas que no son necesarias para hacer predicciones\n",
        "churn_df=churn_df.drop(['RowNumber','CustomerId','Surname'], axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvE88qUjtP9s"
      },
      "source": [
        "Script 4:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zntYHfs7tRZH"
      },
      "outputs": [],
      "source": [
        "#Se divide el DataFrame entre las columnas a predecir (y) y el resto (x)\n",
        "x=churn_df.drop(['Exited'], axis=1)\n",
        "y=churn_df['Exited']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU_GrbpquKxJ"
      },
      "source": [
        "Script 5:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqILdEr_uMaf"
      },
      "outputs": [],
      "source": [
        "x.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hGMyYsUuTF5"
      },
      "source": [
        "Script 6:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uj04pudOuVjH"
      },
      "outputs": [],
      "source": [
        "y.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdZ8o8-hukO6"
      },
      "source": [
        "Script 7:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOc8ffaDulmq"
      },
      "outputs": [],
      "source": [
        "#Separar las columnas con numeros del Dataframe original\n",
        "numerical=x.drop(['Geography','Gender'],axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyqA5OFKu-hL"
      },
      "source": [
        "Script 8:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgVp46ftvATv"
      },
      "outputs": [],
      "source": [
        "numerical.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kf0TwyhrvGVx"
      },
      "source": [
        "Script 9:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4voT3jbivH8f"
      },
      "outputs": [],
      "source": [
        "#Separar las columnas categoricas dle DataFrame original\n",
        "categorical=x.filter(['Geography','Gender'])\n",
        "categorical.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mK509_6xvbeN"
      },
      "source": [
        "Script 10:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrU0hw-Vvc6I"
      },
      "outputs": [],
      "source": [
        "#Transformar el DataFrame de datos categoricos en 1 y 0\n",
        "cat_numerical=pd.get_dummies(categorical, drop_first=True, )\n",
        "#cat_numerical=pd.get_dummies(categorical, drop_first=True,dtype=int)\n",
        "cat_numerical.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh_CioqywPJa"
      },
      "source": [
        "Script 11:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBkaLR5qwQq6"
      },
      "outputs": [],
      "source": [
        "#Unir los DataFrame que solo son numericos para el analisis\n",
        "x=pd.concat([numerical,cat_numerical],axis=1)\n",
        "x.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih3-aH8WwsKx"
      },
      "source": [
        "Script 12:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdbAMUNDwtZz"
      },
      "outputs": [],
      "source": [
        "#Dividir el DataFrame en datos para entrenar (train) y para testear el modelo (test)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#test size is the fraction of the test size\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dSDsRHFxfNt"
      },
      "source": [
        "Script 13:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R22L959WxgpB"
      },
      "outputs": [],
      "source": [
        "#Normalizar los datos del DataFrame\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc=StandardScaler()\n",
        "x_train=sc.fit_transform(x_train)\n",
        "x_test=sc.transform(x_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0VtXhpr012F"
      },
      "source": [
        "Script 14:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRDHgOch03UK"
      },
      "outputs": [],
      "source": [
        "#--LogisticRegression--\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "log_clf=LogisticRegression()\n",
        "#train\n",
        "classifier=log_clf.fit(x_train,y_train)\n",
        "#test\n",
        "y_pred=classifier.predict(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rRU_5sY4yBL"
      },
      "source": [
        "Script 15:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MCVgnjz4zrY"
      },
      "outputs": [],
      "source": [
        "#Determinar la presicion del modelo\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "con_mat=confusion_matrix(y_test,y_pred)\n",
        "cla_rep=classification_report(y_test,y_pred)\n",
        "acc_sco=accuracy_score(y_test,y_pred)\n",
        "print('Confusion matrix: \\n',con_mat)\n",
        "print('Classification report: \\n',cla_rep)\n",
        "print('Accuracy: ',acc_sco)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQMkexZU77ND"
      },
      "outputs": [],
      "source": [
        "#Grafico de Matriz de Confusion\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=con_mat,display_labels=log_clf.classes_)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "#Grafico Presicion vs. Recall\n",
        "from sklearn.metrics import (precision_recall_curve, PrecisionRecallDisplay)\n",
        "PrecisionRecallDisplay.from_predictions(y_test, y_pred)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfeDG03RAO1m"
      },
      "source": [
        "Script 16:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBqyRTRAARXl"
      },
      "outputs": [],
      "source": [
        "#--KNeighborsClassifier--\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn_clf=KNeighborsClassifier(n_neighbors=5)\n",
        "#train\n",
        "classifier=knn_clf.fit(x_train,y_train)\n",
        "#test\n",
        "y_pred=classifier.predict(x_test)\n",
        "\n",
        "#Determinar la presicion del modelo\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "con_mat=confusion_matrix(y_test,y_pred)\n",
        "cla_rep=classification_report(y_test,y_pred)\n",
        "acc_sco=accuracy_score(y_test,y_pred)\n",
        "print('Confusion matrix: \\n',con_mat)\n",
        "print('Classification report: \\n',cla_rep)\n",
        "print('Accuracy: ',acc_sco)\n",
        "\n",
        "#Grafico de Matriz de Confusion\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=con_mat,display_labels=knn_clf.classes_)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "#Grafico Presicion vs. Recall\n",
        "from sklearn.metrics import (precision_recall_curve, PrecisionRecallDisplay)\n",
        "PrecisionRecallDisplay.from_predictions(y_test, y_pred)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvtbAFTPGXUs"
      },
      "source": [
        "Script 17:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgZGPzrsGYnr"
      },
      "outputs": [],
      "source": [
        "#--RandomForestClassifier--\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_clf=RandomForestClassifier(random_state=42,n_estimators=500)\n",
        "\n",
        "#train\n",
        "classifier=rf_clf.fit(x_train,y_train)\n",
        "#test\n",
        "y_pred=classifier.predict(x_test)\n",
        "\n",
        "#Determinar la presicion del modelo\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "con_mat=confusion_matrix(y_test,y_pred)\n",
        "cla_rep=classification_report(y_test,y_pred)\n",
        "acc_sco=accuracy_score(y_test,y_pred)\n",
        "print('Confusion matrix: \\n',con_mat)\n",
        "print('Classification report: \\n',cla_rep)\n",
        "print('Accuracy: ',acc_sco)\n",
        "\n",
        "#Grafico de Matriz de Confusion\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=con_mat,display_labels=rf_clf.classes_)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "#Grafico Presicion vs. Recall\n",
        "from sklearn.metrics import (precision_recall_curve, PrecisionRecallDisplay)\n",
        "PrecisionRecallDisplay.from_predictions(y_test, y_pred)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91eqbogfI8V3"
      },
      "source": [
        "Script 18:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGxxLKYWI-S0"
      },
      "outputs": [],
      "source": [
        "#--SVM--\n",
        "from sklearn import svm\n",
        "\n",
        "svm_clf=svm.SVC()\n",
        "\n",
        "#train\n",
        "classifier=svm_clf.fit(x_train,y_train)\n",
        "#test\n",
        "y_pred=classifier.predict(x_test)\n",
        "\n",
        "#Determinar la presicion del modelo\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "con_mat=confusion_matrix(y_test,y_pred)\n",
        "cla_rep=classification_report(y_test,y_pred)\n",
        "acc_sco=accuracy_score(y_test,y_pred)\n",
        "print('Confusion matrix: \\n',con_mat)\n",
        "print('Classification report: \\n',cla_rep)\n",
        "print('Accuracy: ',acc_sco)\n",
        "\n",
        "#Grafico de Matriz de Confusion\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=con_mat,display_labels=svm_clf.classes_)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "#Grafico Presicion vs. Recall\n",
        "from sklearn.metrics import (precision_recall_curve, PrecisionRecallDisplay)\n",
        "PrecisionRecallDisplay.from_predictions(y_test, y_pred)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5df2V9B6KAs8"
      },
      "source": [
        "Script 19:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaXy2Lj-KCQC"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "print(cross_val_score(classifier,x,y,cv=5,scoring='accuracy'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEzZjllbKYkG"
      },
      "source": [
        "Script 20:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXmeYOvlKams"
      },
      "outputs": [],
      "source": [
        "#Para predecir un valor solo\n",
        "churn_df.loc[100]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yc8W83lKvNG"
      },
      "source": [
        "Script 21:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qcGPHdtKwiJ"
      },
      "outputs": [],
      "source": [
        "#entrenar el modelo con RandomForest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_clf=RandomForestClassifier(random_state=42,n_estimators=500)\n",
        "classifier=rf_clf.fit(x_train,y_train)\n",
        "\n",
        "# scaling single record\n",
        "single_record=sc.transform(x.values[100].reshape(1,-1))\n",
        "\n",
        "#making predictions on the single record\n",
        "predicted_churn = classifier.predict(single_record)\n",
        "print(predicted_churn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIPNpBG8Loql"
      },
      "source": [
        "**Exercise 7.1**\n",
        "\n",
        "\n",
        "**Question 1**\n",
        "\n",
        "Among the following, which one is not an example of classification outputs?\n",
        "\n",
        "A. True\n",
        "\n",
        "B. Red\n",
        "\n",
        "C. Male\n",
        "\n",
        "**D. None of the above**\n",
        "\n",
        "\n",
        "\n",
        "**Question 2**\n",
        "\n",
        "Which of the following metrics is used for unbalanced classification datasets?\n",
        "\n",
        "A. Accuracy\n",
        "\n",
        "B. F1\n",
        "\n",
        "**C. Precision**\n",
        "\n",
        "D. Recall\n",
        "\n",
        "\n",
        "\n",
        "**Question 3**\n",
        "\n",
        "Among the following functions, which one is used to convert categorical values to one-hot encoded numerical values?\n",
        "\n",
        "A. pd.get_onehot()\n",
        "\n",
        "**B. pd.get_dummies()**\n",
        "\n",
        "C. pd.get_numeric()\n",
        "\n",
        "D. All of the above\n",
        "\n",
        "\n",
        "\n",
        "**Exercise 7.2**\n",
        "\n",
        "Using the iris dataset from the Seaborn library, train a classification algorithm of your choice, which predicts the species of the iris plant. Perform all the preprocessing steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3O-OQCLyPjm"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "iris_df=sns.load_dataset('iris')\n",
        "iris_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vocYt4U8yaWc"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import (precision_recall_curve, PrecisionRecallDisplay)\n",
        "\n",
        "#step 1: crear data frame de datos\n",
        "iris_df=sns.load_dataset('iris')\n",
        "\n",
        "\n",
        "#step 2: generar las listas del data frame x e y\n",
        "x=iris_df.drop(['species'],axis=1)\n",
        "y=iris_df['species']\n",
        "\n",
        "#step 3: genear el data frame de numeros\n",
        "  #step 3.1: separar el texto de numeros\n",
        "\n",
        "\n",
        "  #step 3.2: convertir el texto a numeros que se puedan manejar\n",
        "\n",
        "\n",
        "  #step 3.3: unir los 2 dataframe de solo numeros\n",
        "\n",
        "\n",
        "\n",
        "#step 4: generar los datos para entrenamiento y testeo\n",
        "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=0)\n",
        "\n",
        "#step 5: normalizar los valores\n",
        "sc=StandardScaler()\n",
        "  #scaling the training set\n",
        "x_train=sc.fit_transform(x_train)\n",
        "  #scaling the test set\n",
        "x_test=sc.transform(x_test)\n",
        "\n",
        "\n",
        "#step 6: aplicar los algoritmos\n",
        "\n",
        "#--LogisticRegression--\n",
        "log_clf=LogisticRegression()\n",
        "#train\n",
        "classifier=log_clf.fit(x_train,y_train)\n",
        "#test\n",
        "y_pred_lg=classifier.predict(x_test)\n",
        "\n",
        "\n",
        "#--KNeighborsClassifier--\n",
        "knn_clf=KNeighborsClassifier(n_neighbors=5)\n",
        "#train\n",
        "classifier=knn_clf.fit(x_train,y_train)\n",
        "#test\n",
        "y_pred_kn=classifier.predict(x_test)\n",
        "\n",
        "\n",
        "#--RandomForestClassifier--\n",
        "rf_clf=RandomForestClassifier(random_state=42,n_estimators=500)\n",
        "#train\n",
        "classifier=rf_clf.fit(x_train,y_train)\n",
        "#test\n",
        "y_pred_rf=classifier.predict(x_test)\n",
        "\n",
        "\n",
        "#--SVM--\n",
        "svm_clf=svm.SVC()\n",
        "#train\n",
        "classifier=svm_clf.fit(x_train,y_train)\n",
        "#test\n",
        "y_pred_svm=classifier.predict(x_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "y_pred=[y_pred_lg,y_pred_kn,y_pred_rf,y_pred_svm]\n",
        "modelo=[log_clf,knn_clf,rf_clf,svm_clf]\n",
        "algoritmo=['--LogisticRegression--','--KNeighborsClassifier--','--RandomForestClassifier--','--SVM--']\n",
        "\n",
        "for x in range(0,4):\n",
        "  #Determinar la presicion del modelo\n",
        "  print('--',algoritmo[x],'--')\n",
        "  con_mat=confusion_matrix(y_test,y_pred[x])\n",
        "  cla_rep=classification_report(y_test,y_pred[x])\n",
        "  acc_sco=accuracy_score(y_test,y_pred[x])\n",
        "  print('Confusion matrix: \\n',con_mat)\n",
        "  print('Classification report: \\n',cla_rep)\n",
        "  print('Accuracy: ',acc_sco)\n",
        "\n",
        "  #Grafico de Matriz de Confusion\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=con_mat,display_labels=modelo[x].classes_)\n",
        "  disp.plot()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8Sgjo7o3IWz"
      },
      "source": [
        "RESPUESTA DEL LIBRO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCMNWxkR2qcG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "iris_df = sns.load_dataset(\"iris\")\n",
        "\n",
        "iris_df.head()\n",
        "\n",
        "X = iris_df.drop(['species'], axis=1)\n",
        "y = iris_df[\"species\"]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform (X_test)\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_clf = RandomForestClassifier(random_state=42, n_estimators=500)\n",
        "\n",
        "classifier = rf_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "con_mat=confusion_matrix(y_test,y_pred)\n",
        "cla_rep=classification_report(y_test,y_pred)\n",
        "acc_sco=accuracy_score(y_test,y_pred)\n",
        "print('Confusion matrix: \\n',con_mat)\n",
        "print('Classification report: \\n',cla_rep)\n",
        "print('Accuracy: ',acc_sco)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5N28z3fmNYX"
      },
      "source": [
        "# **8**\n",
        "\n",
        "# **Data Clustering with Machine Learning Using Sklearn Library**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LfTQl1RmQbV"
      },
      "source": [
        "Script 1:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4HN7YCfmUEe"
      },
      "outputs": [],
      "source": [
        "#Importing the libraries needed is the first step\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wenqDZ7ym5dq"
      },
      "source": [
        "Script 2:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFnJ25Whn9Nu"
      },
      "outputs": [],
      "source": [
        "#Next, we create a dummy dataset containing 500 records and 4 cluster centers. The average standard deviation between the records is 2.0.\n",
        "#The following script creates a dummy dataset and plots data points on a plot.\n",
        "\n",
        "#generating dummydata of 500 records whit 4 cluster\n",
        "from sklearn.datasets import make_blobs\n",
        "features,cluster=make_blobs(n_samples=500,centers=4,cluster_std=2.00)\n",
        "\n",
        "#plotting the dummy data\n",
        "plt.scatter(features[:,0],features[:,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n75utwufpK3O"
      },
      "source": [
        "Script 3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_qkwA8DpMWU"
      },
      "outputs": [],
      "source": [
        "#performing kmeans clustering using KMeans class\n",
        "km_model=KMeans(n_clusters=4)\n",
        "km_model.fit(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik03L66spocM"
      },
      "source": [
        "Script 4:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmAtlUYgpp0Q"
      },
      "outputs": [],
      "source": [
        "#printing centroid values\n",
        "print(km_model.cluster_centers_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IulxcDmhqHHc"
      },
      "source": [
        "Script 5:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jkqmjp3tqIhT"
      },
      "outputs": [],
      "source": [
        "#For instance, in the case of four clusters, the cluster ids are 0,1,2,3.\n",
        "#To print the cluster ids for all the labels, you can use the labels_attribute of the KMeans class, as shown below.\n",
        "\n",
        "#printing predicted label values\n",
        "print(km_model.labels_)\n",
        "print(len(km_model.labels_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXIHSxZlqlaq"
      },
      "source": [
        "Script 6:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebF5W64ZqnE5"
      },
      "outputs": [],
      "source": [
        "#The following script prints the clusters in different colors along with the cluster centers as black data points, as shown below.\n",
        "\n",
        "#pring the data point\n",
        "plt.scatter(features[:,0],features[:,1],c=km_model.labels_,cmap='rainbow')\n",
        "\n",
        "#print the centroids\n",
        "plt.scatter(km_model.cluster_centers_[:,0], km_model.cluster_centers_[:,1],c='black')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWrlwui4HpFP"
      },
      "source": [
        "Script 7:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmQwx0TLHqbr"
      },
      "outputs": [],
      "source": [
        "#The following script prints the actual four clusters in the dataset.\n",
        "\n",
        "#print actual datapoint\n",
        "plt.scatter(features[:,0],features[:,1],c=km_model.labels_,cmap='rainbow')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFikf8-oIg-N"
      },
      "source": [
        "Script 8:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33TvcXMiIiyg"
      },
      "outputs": [],
      "source": [
        "#The Iris dataset can be imported via the following script\n",
        "import seaborn as sns\n",
        "\n",
        "iris_df=sns.load_dataset('iris')\n",
        "iris_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdqkLIiOI9U6"
      },
      "source": [
        "Script 9:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWUrqG_nI-yA"
      },
      "outputs": [],
      "source": [
        "#We do not use data labels for clustering. Hence, we will separate features from labels\n",
        "\n",
        "#dividing data into features and labels\n",
        "\n",
        "features=iris_df.drop(['species'],axis=1)\n",
        "lables=iris_df.filter(['species'],axis=1)\n",
        "features.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo1ddmeFJi6Y"
      },
      "source": [
        "Script 10:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTckyxS1JklW"
      },
      "outputs": [],
      "source": [
        "#Lets first choose 4 as a random number for the number of clusters.\n",
        "#The following script performs K Means clustering on the Iris dataset\n",
        "\n",
        "#trainig KMeans model\n",
        "#features=features.values()\n",
        "km_model=KMeans(n_clusters=4)\n",
        "km_model.fit(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXdJrO_CK8p1"
      },
      "source": [
        "Script 11:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylFgsKJuK9_R"
      },
      "outputs": [],
      "source": [
        "#To print labels of the Iris dataset, execute the following script\n",
        "\n",
        "print(km_model.labels_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmTtiBRoLImX"
      },
      "source": [
        "Script 12:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEwsFl8-LKKm"
      },
      "outputs": [],
      "source": [
        "#Finally, to plot the 4 clusters found by the K Means algorithm in the Iris dataset, along with the predicted cluster centroids, execute the following script\n",
        "\n",
        "#print the datapoint\n",
        "plt.scatter(features[:,0],features[:,1],c=km_model.labels_,cmap='rainbow')\n",
        "\n",
        "#print the centroids\n",
        "plt.scatter(km_model.cluster_centers_[:,0],km_model.cluster_centers_[:,1],s=100,c='black')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASfiP6fNMn1G"
      },
      "source": [
        "Script 13:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OndOP_i_MpM6"
      },
      "outputs": [],
      "source": [
        "#To calculate the inertia value, you can use the inertia_ attribute of the KMeans class object.\n",
        "#The following script creates inertial values for K=1 to 10 and plots in the form of a line plot, as shown below\n",
        "\n",
        "#trainig KMeans on K values from 1 to 10\n",
        "loss=[]\n",
        "for i in range(1,11):\n",
        "  km=KMeans(n_clusters=i).fit(features)\n",
        "  loss.append(km.inertia_)\n",
        "\n",
        "#printing loss against number of clusters\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(range(1,11),loss)\n",
        "plt.title('Finding Optimal Clusters via Elbow Method')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmzogpD4N-_m"
      },
      "source": [
        "From the output below, it can be seen that the value of inertia didnt decrease much after 3 clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kdo2DNPmObtW"
      },
      "source": [
        "Script 14:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TB2XYWvFODy2"
      },
      "outputs": [],
      "source": [
        "#Lets now cluster the Iris data using 3 clusters and see if we can get close to the actual clusters\n",
        "\n",
        "#training KMeans with 3 clusters\n",
        "km_model=KMeans(n_clusters=3)\n",
        "km_model.fit(features)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDVZpPAsOdaS"
      },
      "source": [
        "Script 15:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb5mhF1gOfDE"
      },
      "outputs": [],
      "source": [
        "# print the datapoint with prediced labels\n",
        "plt.scatter(features[:,0],features[:,1],c=km_model.labels_,cmap='rainbow')\n",
        "\n",
        "#print the predicted centroids\n",
        "plt.scatter(km_model.cluster_centers_[:,0],km_model.cluster_centers_[:,1],s=100,c='black')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfUBJz-mOX3w"
      },
      "source": [
        "Script 16:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0L5luyHXPV7o"
      },
      "outputs": [],
      "source": [
        "#Lets now plot the actual clusters and see how close the actual clusters are to predicted clusters\n",
        "\n",
        "#converting categorical labels to numbers\n",
        "#from sklearn import preprocessing\n",
        "#le=preprocessing.LabelEncoder()\n",
        "#labels=le.fit_transform(labels)\n",
        "\n",
        "#print the data poin with the orgonal labels\n",
        "plt.scatter(features[:,0], features[:,1], c=km_model.labels_,cmap='rainbow')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rPQLri8UIlu"
      },
      "source": [
        "Script 17:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQHx1HTFU6UY"
      },
      "outputs": [],
      "source": [
        "#The following script imports the required libraries:\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_blobs\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq3HgmtSVi6Q"
      },
      "source": [
        "Script 18:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64sLQQzVVkMz"
      },
      "outputs": [],
      "source": [
        "#The following script randomly creates data points and then labels the data points from 1 to 10. The data points are plotted as a scatter plot.\n",
        "\n",
        "#generating dummy data of 10 records with 2 clusters\n",
        "features, labels=make_blobs(n_samples=10,centers=2,cluster_std=2)\n",
        "print('FEATURES: ',features)\n",
        "print('LABELS: ', labels)\n",
        "\n",
        "#plotting the dummy data\n",
        "plt.scatter(features[:,0], features[:,1],color='r')\n",
        "\n",
        "#adding number to data points\n",
        "annots=range(1,11)\n",
        "for label,x,y in zip(annots,features[:,0],features[:,1]):\n",
        "  plt.annotate(label,xy=(x,y),xytext=(-3,3),textcoords='offset points',ha='right',va='bottom')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7CYBDgwZQPd"
      },
      "source": [
        "Script 19:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmTfOEPiZRiu"
      },
      "outputs": [],
      "source": [
        "#Lets now plot dendrograms for the above 10 data points. To\n",
        "#plot dendrograms, you can use the dendrogram and linkage\n",
        "#classes from the scipy.cluster.hierarchy module. The features\n",
        "#are passed to the linkage class. And the object of the linkage\n",
        "#class is passed to the dendrogram class to plot dendrogram\n",
        "#for the features, as shown in the following script:\n",
        "\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "dendos=linkage(features, 'single')\n",
        "\n",
        "annots=range(1,11)\n",
        "\n",
        "dendrogram(dendos, orientation='top', labels=annots,distance_sort='descending',show_leaf_counts=True)\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4Y6fvXLbOEY"
      },
      "source": [
        "Script 20:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4F5HsZCbPk7"
      },
      "outputs": [],
      "source": [
        "#The following script applies agglomerative clustering to our dummy dataset.\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "#trainingagglomerative clustering model\n",
        "hc_model =AgglomerativeClustering(n_clusters=3,affinity='euclidean',linkage='ward')\n",
        "hc_model.fit(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ux8I-z7b3pF"
      },
      "source": [
        "Script 21:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p33av9Zcb451"
      },
      "outputs": [],
      "source": [
        "#And the following script plots the predicted clusters.\n",
        "\n",
        "#print the data point\n",
        "plt.scatter(features[:,0],features[:,1],c=hc_model.labels_,cmap='rainbow')\n",
        "#adding number to data points\n",
        "annots=range(1,11)\n",
        "for label,x,y in zip(annots,features[:,0],features[:,1]):\n",
        "  plt.annotate(label,xy=(x,y),xytext=(-3,3),textcoords='offset points',ha='right',va='bottom')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2vcRe-cVN77"
      },
      "source": [
        "Script 22:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKr5BsW0VUKM"
      },
      "outputs": [],
      "source": [
        "#The following script creates 500 data points with 4 cluster centers.\n",
        "\n",
        "#generating dummy data of 500 records with 4 clusters\n",
        "features, labels=make_blobs(n_samples=500,centers=4,cluster_std=2)\n",
        "\n",
        "#plotting the dummy data\n",
        "plt.scatter(features[:,0],features[:,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvEDfLoFWWHq"
      },
      "source": [
        "Script 23:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7YrS3aoWXd6"
      },
      "outputs": [],
      "source": [
        "#The following script applies agglomerative hierarchical clustering on the dataset. The number of predicted clusters is 4.\n",
        "\n",
        "#performing kmeans clustering using AgglomerativeClustering class\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "hc_model=AgglomerativeClustering(n_clusters=4,affinity='euclidean',linkage='ward')\n",
        "hc_model.fit_predict(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGv0aoscXWWg"
      },
      "source": [
        "Script 24:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQweagp1XXhO"
      },
      "outputs": [],
      "source": [
        "#To plot the predicted clusters, execute the following script.\n",
        "\n",
        "#print the data point\n",
        "\n",
        "plt.scatter(features[:,0],features[:,1],c=hc_model.labels_,cmap='rainbow')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehih4wL6YVlF"
      },
      "source": [
        "Script 25:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xF51U19bYWwB"
      },
      "outputs": [],
      "source": [
        "#Similarly, to plot the actual clusters in the dataset (for the sake of comparison), execute the following script.\n",
        "\n",
        "#print actual data point\n",
        "plt.scatter(features[:,0], features[:,1],c=labels, cmap='rainbow')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVI4WzY8Y2Y1"
      },
      "source": [
        "Script 26:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LFTXsHxaLKt"
      },
      "outputs": [],
      "source": [
        "#The following script imports the Iris dataset and displays the first five rows of the dataset.\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "iris_df=sns.load_dataset('iris')\n",
        "iris_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yggvD2Nlae7h"
      },
      "source": [
        "Script 27:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXU_hcj4af-7"
      },
      "outputs": [],
      "source": [
        "#The following script divides the data into features and labels sets and displays the first five rows of the labels set.\n",
        "\n",
        "#dividing data into features and labels\n",
        "\n",
        "features=iris_df.drop(['species'],axis=1)\n",
        "labels=iris_df.filter(['species'],axis=1)\n",
        "features.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl7bH8yPa6bP"
      },
      "source": [
        "Script 28:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWTRPDQ4a8Dq"
      },
      "outputs": [],
      "source": [
        "#Similarly, the following script applies the agglomerative clustering on the feature set using the AgglomerativeClustering class from the sklearn.cluster module.\n",
        "\n",
        "#training hierarchical clustering model\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "#traning agglomerative clustering model\n",
        "features=features.values\n",
        "hc_model=AgglomerativeClustering(n_clusters=3,affinity='euclidean',linkage='ward')\n",
        "hc_model.fit_predict(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlqFuRFGbsyE"
      },
      "source": [
        "Script 29:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sfXLmQ5bt0K"
      },
      "outputs": [],
      "source": [
        "#The predicted clusters are printed via the following script.\n",
        "\n",
        "#print the data point\n",
        "\n",
        "plt.scatter(features[:,0],features[:,1],c=hc_model.labels_,cmap='rainbow')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRmLN5p2b_rP"
      },
      "source": [
        "Script 30:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__L_qIcGcA8R"
      },
      "outputs": [],
      "source": [
        "import scipy.cluster.hierarchy as shc\n",
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.title('Iris dendograms')\n",
        "\n",
        "dend=shc.dendrogram(shc.linkage(features,method='ward'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy0WAl-Kcmf3"
      },
      "source": [
        "**Exercise 8.1**\n",
        "\n",
        "**Question 1**\n",
        "\n",
        "Which of the following is a supervised machine learning algorithm?\n",
        "\n",
        "A. K Means Clustering\n",
        "\n",
        "B. Hierarchical Clustering\n",
        "\n",
        "C. All of the above\n",
        "\n",
        "**D. None of the above**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Question 2\n",
        "\n",
        "In KMeans clustering, what does the inertia tell us?\n",
        "\n",
        "A. the distance between data points within cluster\n",
        "\n",
        "B. output labels for the data points\n",
        "\n",
        "**C. the number of clusters**\n",
        "\n",
        "D. None of the above\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Question 3\n",
        "\n",
        "In hierarchical clustering, in the case of vertical dendrograms, the number of clusters is equal to the number of ____ lines that the ____ line passes through?\n",
        "\n",
        "A. horizontal, vertical\n",
        "\n",
        "**B. vertical, horizontal**\n",
        "\n",
        "C. none of the above\n",
        "\n",
        "D. All of the above\n",
        "\n",
        "\n",
        "\n",
        "**Exercise 8.2**\n",
        "\n",
        "Apply KMeans clustering on the banknote.csv dataset available in the Data folder in the book resources. Find the optimal number of clusters and then print the clustered dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kTT4hTkwxn8i"
      },
      "outputs": [],
      "source": [
        "#STEP 1: IMPORT LIBRARIES\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#STEP 2: IMPORT DATA\n",
        "data_df=pd.read_csv('banknote.csv')\n",
        "\n",
        "\n",
        "#STEP 3: DIVIDING DATA INTO FEATURES AND LABELS\n",
        "features=data_df.drop(['class'],axis=1)\n",
        "labels=data_df.filter(['class'],axis=1)\n",
        "\n",
        "\n",
        "#STEP 4: DETERMINATE THE OPTIMAL NUMBER OF CLUSTER\n",
        "  #trainig KMeans on K values from 1 to 100\n",
        "loss=[]\n",
        "for i in range(1,101):\n",
        "  km=KMeans(n_clusters=i).fit(features)\n",
        "  loss.append(km.inertia_)\n",
        "\n",
        "  #printing loss against number of clusters\n",
        "plt.plot(range(1,101),loss)\n",
        "plt.title('Finding Optimal Clusters via Elbow Method')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "#STEP 5: TRANING KMEANS WITH 20 CLUSTERS\n",
        "features = features.values\n",
        "km_model=KMeans(n_clusters=10)\n",
        "km_model.fit(features)\n",
        "\n",
        "\n",
        "#STEP 6: PRINT THE DATAPOINT\n",
        "  # print the datapoint with prediced labels\n",
        "plt.scatter(features[:,0],features[:,1],c=km_model.labels_,cmap='rainbow')\n",
        "  #print the predicted centroids\n",
        "plt.scatter(km_model.cluster_centers_[:,0],km_model.cluster_centers_[:,1],s=100,c='black')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjBLIR7y956E"
      },
      "source": [
        "# **9**\n",
        "# **Deep Learning with**\n",
        "# **Python TensorFlow 2.0**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imUKydkC-lbT"
      },
      "source": [
        "**9.1. Densely Connected Neural Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCUHxqOz-SCw"
      },
      "source": [
        "Script 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "opWOqJBWCzgi"
      },
      "outputs": [],
      "source": [
        "#The following script upgrades the existing TensorFlow version.\n",
        "\n",
        "pip install --upgrade tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh79CFBsCHUP"
      },
      "source": [
        "Script 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXwmXnd5CIn2"
      },
      "outputs": [],
      "source": [
        "#To check if you are actually running TensorFlow 2.0\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiCklLxVD5ep"
      },
      "source": [
        "Script 3: Importing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LvWBKi8D648"
      },
      "outputs": [],
      "source": [
        "#Lets import the required libraries.\n",
        "\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Dense,Dropout,Activation\n",
        "from tensorflow.keras.models import Model,Sequential\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdD_OLzQEkK7"
      },
      "source": [
        "Script 4: Importing the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byyfl5IWElGR"
      },
      "outputs": [],
      "source": [
        "#Importing the dataset\n",
        "\n",
        "#reading data from csv file\n",
        "banknote_data=pd.read_csv('banknote.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryfQko0GE4Wx"
      },
      "source": [
        "Script 5:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtO4CN4uE5bc"
      },
      "outputs": [],
      "source": [
        "#The following script plots the first five rows of the dataset.\n",
        "\n",
        "banknote_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGChKJ6NFTRy"
      },
      "source": [
        "Script 6:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmgWhN-3FUtU"
      },
      "outputs": [],
      "source": [
        "#Lets see the shape of our dataset\n",
        "\n",
        "banknote_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9880aGaiFvLB"
      },
      "source": [
        "Script 7:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhi5cBLQFwPL"
      },
      "outputs": [],
      "source": [
        "#Lets plot a count plot to see the distribution of data with respect to the values in the class that we want to predict.\n",
        "\n",
        "sns.countplot(x='class', data=banknote_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlCmP8l0GtxW"
      },
      "source": [
        "Script 8:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h825r2lCGu8C"
      },
      "outputs": [],
      "source": [
        "#Lets divide our data into features and target labels.\n",
        "\n",
        "x=banknote_data.drop(['class'],axis=1)\n",
        "y=banknote_data[['class']].values\n",
        "\n",
        "print(x.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwROgwyBHt4e"
      },
      "source": [
        "Script 9: Dividing Data into Training and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFo8fPXpHzlW"
      },
      "outputs": [],
      "source": [
        "#We will divide the total data into an 80 percent training set\n",
        "#and a 20 percent test set. The following script performs that task.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test =train_test_split(x,y,test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSK5ne3eIi3l"
      },
      "source": [
        "Script 10:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IklrZTmtIj83"
      },
      "outputs": [],
      "source": [
        "#Before you train your deep learning model, it is always a\n",
        "#good practice to scale your data. The following script applies\n",
        "#standard scaling to the training and test sets.\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc=StandardScaler()\n",
        "x_train=sc.fit_transform(x_train)\n",
        "x_test=sc.transform(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IyXbWL-I_Yv"
      },
      "source": [
        "Script 11: Creating a Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h40ej4LXJH7e"
      },
      "outputs": [],
      "source": [
        "def create_model (learning_rate,dropout_rate):\n",
        "  #create sequential model\n",
        "  model=Sequential()\n",
        "  #adding dense layers\n",
        "  model.add(Dense(12,input_dim=x_train.shape[1], activation='relu'))\n",
        "  model.add(Dropout(dropout_rate))\n",
        "  model.add(Dense(6,activation='relu'))\n",
        "  model.add(Dropout(dropout_rate))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  #compiling the model\n",
        "  adam=Adam(lr=learning_rate)\n",
        "  model.compile(loss='binary_crossentropy',optimizer=adam,metrics=['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9pT3EkuPbjD"
      },
      "source": [
        "Script 12:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2p1OuzsPdNP"
      },
      "outputs": [],
      "source": [
        "dropout_rate=0.1\n",
        "epochs=20\n",
        "batch_size=4\n",
        "learn_rate=0.001\n",
        "\n",
        "model=create_model(learn_rate,dropout_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83z6ZZ4EP5OO"
      },
      "source": [
        "Script 13:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0QB8iyIP6Vu"
      },
      "outputs": [],
      "source": [
        "#You can see your model architecture via the plot_model() method of the tensorflow.keras.utils module.\n",
        "\n",
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model,to_file='model_plot1.png',show_shapes=True,show_layer_names=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIaLKDprRPNx"
      },
      "source": [
        "Script 14:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhS2NV62RQRp"
      },
      "outputs": [],
      "source": [
        "model_history=model.fit(x_train,y_train,batch_size=batch_size,epochs=epochs,validation_split=0.2,verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeH6l-aEKDSu"
      },
      "source": [
        "Script 15: Evaluating the Neural Network Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrQOB8v9KFag"
      },
      "outputs": [],
      "source": [
        "#To make predictions on the test set, you have to pass\n",
        "#the set to the evaluate() method of the model\n",
        "\n",
        "accuracies=model.evaluate(x_test,y_test,verbose=1)\n",
        "print('Test Score',accuracies[0])\n",
        "print('Test Accuracy',accuracies[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdXwRN9jKvLs"
      },
      "source": [
        "Script 16:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aI54yhtTKxPN"
      },
      "outputs": [],
      "source": [
        "# Lets now plot the accuracy on the training and test sets to see\n",
        "#if our model is overfitting or not.\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.plot(model_history.history['accuracy'],label='accuracy')\n",
        "plt.plot(model_history.history['val_accuracy'],label='val_accuracy')\n",
        "plt.legend(['train','test'],loc='lower left')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgtVAZj-Lwe-"
      },
      "source": [
        "Script 17:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktsqD6cmLx1U"
      },
      "outputs": [],
      "source": [
        "#Similarly, the loss values for test and training sets can be\n",
        "#printed as follows:\n",
        "\n",
        "plt.plot(model_history.history['loss'],label='loss')\n",
        "plt.plot(model_history.history['val_loss'],label='val_loss')\n",
        "plt.legend(['train','test'],loc='upper left')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9f4jIoHIBWT"
      },
      "source": [
        "**9.2. Recurrent Neural Networks (RNN)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkPy34qsL6NV"
      },
      "source": [
        "In this section, we will predict the opening stock price of the\n",
        "Facebook company, using the opening stock price of the\n",
        "previous 60 days."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZezoRd9IDPO"
      },
      "source": [
        "Script 18:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJHz-pCDMDDH"
      },
      "outputs": [],
      "source": [
        "#Before you train the stock market prediction model, upload the TensorFlow version\n",
        "\n",
        "#pip install --upgrade tensorflow\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-ewXVgtM5Iu"
      },
      "source": [
        "Script 20:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8PnzEAKM6rD"
      },
      "outputs": [],
      "source": [
        "#Next, to import the training dataset, execute the following script:\n",
        "\n",
        "#importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#imporing dataset\n",
        "fb_complete_data=pd.read_csv('fb_train.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6H02m7INX4c"
      },
      "source": [
        "Script 21:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZDkcMuwNZIA"
      },
      "outputs": [],
      "source": [
        "#printing dataset header\n",
        "fb_complete_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHVWZf6mNsWy"
      },
      "source": [
        "Script 22:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLfEZWkVNthe"
      },
      "outputs": [],
      "source": [
        "#The output shows that our dataset consists of seven columns.\n",
        "#However, in this section, we are only interested in the Open\n",
        "#column. Therefore, we will select the Open column from the\n",
        "#dataset\n",
        "\n",
        "#filtering open column\n",
        "fb_training_processed=fb_complete_data[['Open']].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEAWDgRwOFO_"
      },
      "source": [
        "Script 23:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcGJ7rKHOGwd"
      },
      "outputs": [],
      "source": [
        "#Next, we will scale our dataset.\n",
        "\n",
        "#scaling features\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler=MinMaxScaler(feature_range=(0,1))\n",
        "\n",
        "fb_training_scaled=scaler.fit_transform(fb_training_processed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xRYQTbNOq7P"
      },
      "source": [
        "Script 24:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhhVNSbNOsMi"
      },
      "outputs": [],
      "source": [
        "print(len(fb_training_scaled))\n",
        "print(fb_training_scaled)\n",
        "print(fb_training_scaled[0,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIP6ZXFOmPsr"
      },
      "source": [
        "Script 25:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPj7ypq3mQ9s"
      },
      "outputs": [],
      "source": [
        "#Based on the opening stock prices of the previous days, we will be predicted the\n",
        "#opening stock price for the next day.\n",
        "\n",
        "#training features contain data of last 60 days\n",
        "#training labels contain data of 61st day\n",
        "\n",
        "fb_training_features=[]\n",
        "fb_training_labels=[]\n",
        "\n",
        "for i in range(60, len(fb_training_scaled)):\n",
        "  fb_training_features.append(fb_training_scaled[i-60:i,0])\n",
        "  fb_training_labels.append(fb_training_scaled[i,0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmTrkDuGqjEu"
      },
      "source": [
        "Script 26:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxjTpDYQqkPP"
      },
      "outputs": [],
      "source": [
        "#We need to convert our data into Numpy array before we can use as input with Keras\n",
        "\n",
        "#converting training data to numpy array\n",
        "x_train=np.array(fb_training_features)\n",
        "y_train=np.array(fb_training_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf26mdEvq4tj"
      },
      "source": [
        "Script 27:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNLiyer0q5wW"
      },
      "outputs": [],
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co_bm6Hqrcjz"
      },
      "source": [
        "Script 28:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afGRKkq4rdxV"
      },
      "outputs": [],
      "source": [
        "#We need to reshape our input features into 3-dimensional format.\n",
        "\n",
        "#converting data into 3d shape\n",
        "x_train=np.reshape(x_train,(x_train.shape[0],x_train.shape[1],1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJXEYJWOsAyL"
      },
      "source": [
        "Script 29:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVuVQtfEsB9V"
      },
      "outputs": [],
      "source": [
        "#importing libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Input,Activation, Dense, Flatten, Dropout, LSTM\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RR1bUQksiuG"
      },
      "source": [
        "Script 30:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XQh_UD-skpu"
      },
      "outputs": [],
      "source": [
        "#defining the LSTM network\n",
        "\n",
        "input_layer=Input(shape=(x_train.shape[1],1))\n",
        "lstm1=LSTM(100,activation='relu', return_sequences=True)(input_layer)\n",
        "do1=Dropout(0.2)(lstm1)\n",
        "\n",
        "lstm2=LSTM(100,activation='relu', return_sequences=True)(do1)\n",
        "do2=Dropout(0.2)(lstm2)\n",
        "\n",
        "lstm3=LSTM(100,activation='relu', return_sequences=True)(do2)\n",
        "do3=Dropout(0.2)(lstm3)\n",
        "\n",
        "lstm4=LSTM(100,activation='relu')(do3)\n",
        "do4=Dropout(0.2)(lstm4)\n",
        "\n",
        "output_layer=Dense(1)(do4)\n",
        "model=Model(input_layer,output_layer)\n",
        "model.compile(optimizer='adam',loss='mse')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIy_Bwq5vCaN"
      },
      "source": [
        "Script 31:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YTv951TvDxw"
      },
      "outputs": [],
      "source": [
        "#Next, we need to convert the output y into a column vector.\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "y_train=y_train.reshape(-1,1)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1BrLhyUvZas"
      },
      "source": [
        "Script 32:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jv__R2dEvasp"
      },
      "outputs": [],
      "source": [
        "#The following script trains our stock price prediction model on the training set.\n",
        "\n",
        "#training model\n",
        "model_history=model.fit(x_train,y_train,epochs=100,verbose=1,batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPcmcb_6wLAB"
      },
      "source": [
        "Script 33:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9ZCoiRVwNdf"
      },
      "outputs": [],
      "source": [
        "#Lets first import the data and then remove all the columns from the test data except the Open column.\n",
        "\n",
        "#creating test set\n",
        "fb_testing_complete_data=pd.read_csv('fb_test.csv')\n",
        "fb_testing_processed=fb_testing_complete_data[['Open']].values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZOuZRiSQAiT"
      },
      "source": [
        "Script 34:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwTjkdHwQB7L"
      },
      "outputs": [],
      "source": [
        "#Lets concatenate the training and test sets.\n",
        "\n",
        "fb_all_data=pd.concat((fb_complete_data['Open'],fb_testing_complete_data['Open']),axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPN-ATcAQgJZ"
      },
      "source": [
        "Script 35:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-w8N-56QhtR"
      },
      "outputs": [],
      "source": [
        "test_input=fb_all_data[len(fb_all_data)-len(fb_testing_complete_data)-60:].values\n",
        "print(test_input.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP2mnEu8RDrG"
      },
      "source": [
        "Script 36:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Thac8w2HRFOz"
      },
      "outputs": [],
      "source": [
        "#We need to scale our data and convert it into a column vector.\n",
        "\n",
        "test_input=test_input.reshape(-1,1)\n",
        "test_input=scaler.transform(test_input)\n",
        "print(test_input.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIRn0LDaRh5A"
      },
      "source": [
        "Script 37:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PX0E4Cx3RjRw"
      },
      "outputs": [],
      "source": [
        "#we need to divide our input data into features and labels.\n",
        "\n",
        "fb_test_features=[]\n",
        "for i in range(60,80):\n",
        "  fb_test_features.append(test_input[i-60:i,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbauImTGR5PG"
      },
      "source": [
        "Script 38:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ptpi9CNpR6vw"
      },
      "outputs": [],
      "source": [
        "x_test=np.array(fb_test_features)\n",
        "print(x_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txy3GWa3SYKs"
      },
      "source": [
        "Script 39:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsCc4eQDSZmg"
      },
      "outputs": [],
      "source": [
        "#Our feature set is currently 2-dimensional. But the LSTM\n",
        "#algorithm in Keras accepts only data in 3-dimensional. The\n",
        "#following script converts our input features into a 3-dimensional\n",
        "#shape.\n",
        "\n",
        "#converting test data into 3d shape\n",
        "\n",
        "x_test=np.reshape(x_test,(x_test.shape[0],x_test.shape[1],1))\n",
        "print(x_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHQSVKgES5ov"
      },
      "source": [
        "Script 40:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOdh0LUBS63v"
      },
      "outputs": [],
      "source": [
        "#Now is the time to make predictions on the test set.\n",
        "\n",
        "#making prediction on test set\n",
        "y_pred=model.predict(x_test)\n",
        "print(y_pred.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0Lx5XydTMgJ"
      },
      "source": [
        "Script 41:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lb5nuMh1TNnY"
      },
      "outputs": [],
      "source": [
        "#Since we scaled our input feature, we need to apply the\n",
        "#inverse_transform() method of the scaler object on the\n",
        "#predicted output to get the original output values\n",
        "\n",
        "#coverting scaled data back to original data\n",
        "#a = np.arange(6).reshape((3, 2))\n",
        "#y_pred=y_pred.reshape(20,60)\n",
        "#print(y_pred.shape)\n",
        "y_pred=scaler.inverse_transform(y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-amRlkqvTiy6"
      },
      "source": [
        "Script 42:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IusLBVLUTkKL"
      },
      "outputs": [],
      "source": [
        "#plotting original and predicted stock values\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fb_testing_processed, color='red',label='Actual Facebook Stock Price')\n",
        "plt.plot(y_pred,color='green',label='Predicted Facebook Stock Price')\n",
        "plt.title('Facebook Stock Price')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Stock Price')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSaOHJRq8AzS"
      },
      "source": [
        "**9.3. Convolutional Neural Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWAA0jFi8CF2"
      },
      "source": [
        "Script 43:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nM3c_2os-INm"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bojNgX8n-WkY"
      },
      "source": [
        "Script 44:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FDGzQS6-X5e"
      },
      "outputs": [],
      "source": [
        "#importing required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout, MaxPool2D\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5Ngsy3F-9Ul"
      },
      "source": [
        "Script 45:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qW4oPA4--jZ"
      },
      "outputs": [],
      "source": [
        "#The following script downloads the Fashion MNIST dataset\n",
        "#that contains images of different fashion items along with\n",
        "#their labels. The script divides the data into training images\n",
        "#and training labels and test images and test labels.\n",
        "\n",
        "#importing mnist dataset\n",
        "mnist_data=tf.keras.datasets.fashion_mnist\n",
        "\n",
        "#dividing data into training and test set\n",
        "(training_images, training_labels),(test_images, test_labels)=mnist_data.load_data()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJms9m2v_uYZ"
      },
      "source": [
        "Script 46:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rg1slBmb_vgg"
      },
      "outputs": [],
      "source": [
        "#The images in our dataset are greyscale images, where each\n",
        "#pixel value lies between 0 and 255. The following script\n",
        "#normalizes pixel values between 0 and 1.\n",
        "\n",
        "#scaling images\n",
        "training_images,test_images=training_images/255, test_images/255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmWLYfEvACbd"
      },
      "source": [
        "Script 47:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTYh-SrFADvm"
      },
      "outputs": [],
      "source": [
        "print(training_images.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u58Md41XARbd"
      },
      "source": [
        "Script 48:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZtdWtZDASiE"
      },
      "outputs": [],
      "source": [
        "#Lets print an image randomly from the test set:\n",
        "\n",
        "#plotting image number 9 from test set\n",
        "plt.figure()\n",
        "plt.imshow(test_images[5])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2o5q2BIA26N"
      },
      "source": [
        "Script 49:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysBEoJ5hA39_"
      },
      "outputs": [],
      "source": [
        "#converting data into the right shape\n",
        "training_images=np.expand_dims(training_images,-1)\n",
        "test_images=np.expand_dims(test_images,-1)\n",
        "print(training_images.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piabXZBnBfL5"
      },
      "source": [
        "Script 50:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXFsjW_qBgS-"
      },
      "outputs": [],
      "source": [
        "#The next step is to find the number of output classes. This\n",
        "#number will be used to define the number of neurons in the\n",
        "#output layer.\n",
        "\n",
        "#printing number of output calsses\n",
        "output_calsses=len(set(training_labels))\n",
        "print('Number of output classes is: ',output_calsses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVQr2kBeB7FG"
      },
      "source": [
        "Script 51:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m1sRQyVB8Jh"
      },
      "outputs": [],
      "source": [
        "training_images[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bu5ppDUCJf_"
      },
      "source": [
        "Script 52:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTc7lSjiCK69"
      },
      "outputs": [],
      "source": [
        "#Developing the CNN model\n",
        "\n",
        "input_layer = Input(shape = training_images[0].shape)\n",
        "conv1 = Conv2D(32, (3,3), strides = 2, activation= 'relu')(input_layer)\n",
        "maxpool1 = MaxPool2D(2, 2)(conv1)\n",
        "conv2 = Conv2D(64, (3,3), strides = 2, activation= 'relu')(maxpool1)\n",
        "#conv3 = Conv2D(128, (3,3), strides = 2, activation='relu')(conv2)\n",
        "flat1 = Flatten()(conv2)\n",
        "drop1 = Dropout(0.2)(flat1)\n",
        "dense1 = Dense(512, activation = 'relu')(drop1)\n",
        "drop2 = Dropout(0.2)(dense1)\n",
        "output_layer = Dense(output_calsses, activation='softmax')(drop2)\n",
        "\n",
        "model = Model(input_layer, output_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ihs2kg1uC7wF"
      },
      "source": [
        "Script 53:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjJUab8pC-BJ"
      },
      "outputs": [],
      "source": [
        "#compiling the CNN model\n",
        "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jw3p4aPPDRUK"
      },
      "source": [
        "Script 54:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMb3pUzhDSmQ"
      },
      "outputs": [],
      "source": [
        "#Finally, execute the following script to print the model architecture.\n",
        "\n",
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model,to_file='model_plot1.png',show_shapes=True,show_layer_names=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gWUJ748D4rV"
      },
      "source": [
        "Script 55:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14HRf8aqD5mZ"
      },
      "outputs": [],
      "source": [
        "#The following script trains the image classification model.\n",
        "\n",
        "#training the CNN model\n",
        "model_history=model.fit(training_images,training_labels,epochs=20,validation_data=(test_images,test_labels),verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFisH3NnEXre"
      },
      "source": [
        "Script 56:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bj-NJNa6EZVl"
      },
      "outputs": [],
      "source": [
        "#plotting accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(model_history.history['accuracy'],label='accuracy')\n",
        "plt.plot(model_history.history['val_accuracy'],label='val_accuracy')\n",
        "plt.legend(['train','test'],loc='lower left')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MjIi_jJFIbo"
      },
      "source": [
        "Script 57:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHxW46GBFJop"
      },
      "outputs": [],
      "source": [
        "#making predictions on a single image\n",
        "output=model.predict(test_images)\n",
        "prediction=np.argmax(output[5])\n",
        "print( prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7JXI35lHYsI"
      },
      "source": [
        "**Exercise 9.1**\n",
        "\n",
        "**Question 1**\n",
        "\n",
        "What should be the input shape of the input image to the convolutional neural network?\n",
        "\n",
        "A. Width, Height\n",
        "\n",
        "B. Height, Width\n",
        "\n",
        "C. Channels, Width, Height\n",
        "\n",
        "**D. Width, Height, Channels**\n",
        "\n",
        "\n",
        "**Question 2**\n",
        "\n",
        "We say that a model is overfitting when:\n",
        "\n",
        "A. Results on test set are better than train set\n",
        "\n",
        "B. Results on both test and training sets are similar\n",
        "\n",
        "**C. Results on the training set are better than the results on the test set**\n",
        "\n",
        "D. None of the above\n",
        "\n",
        "\n",
        "**Question 3**\n",
        "\n",
        "The ReLu activation function is used to introduce:\n",
        "\n",
        "A. Linearity\n",
        "\n",
        "**B. Non-linearity**\n",
        "\n",
        "C. Quadraticity\n",
        "\n",
        "D. None of the above\n",
        "\n",
        "\n",
        "**Exercise 9.2**\n",
        "\n",
        "Using the CFAR 10 image dataset, perform image classification to recognize the image. Here is the dataset:\n",
        "1. cifar_dataset = tf.keras.datasets.cifar10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncs6AKDTM5BF"
      },
      "outputs": [],
      "source": [
        "#importing required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Input, Conv2D, Dense,Flatten, Dropout, MaxPool2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "cifar_dataset = tf.keras.datasets.cifar10\n",
        "(training_images, training_labels), (test_images, test_labels) = cifar_dataset.load_data()\n",
        "training_images, test_images = training_images/255.0,test_images/255.0\n",
        "\n",
        "training_labels, test_labels = training_labels.flatten(),test_labels.flatten()\n",
        "print(training_labels.shape)\n",
        "print(training_images.shape)\n",
        "output_classes = len(set(training_labels))\n",
        "print(\"Number of output classes is: \", output_classes)\n",
        "input_layer = Input(shape = training_images[0].shape )\n",
        "conv1 = Conv2D(32, (3,3), strides = 2, activation= 'relu')(input_layer)\n",
        "maxpool1 = MaxPool2D(2, 2)(conv1)\n",
        "conv2 = Conv2D(64, (3,3), strides = 2, activation= 'relu')(maxpool1)\n",
        "#conv3 = Conv2D(128, (3,3), strides = 2, activation='relu')(conv2)\n",
        "flat1 = Flatten()(conv2)\n",
        "drop1 = Dropout(0.2)(flat1)\n",
        "dense1 = Dense(512, activation = 'relu')(drop1)\n",
        "drop2 = Dropout(0.2)(dense1)\n",
        "output_layer = Dense(output_classes, activation='softmax')(drop2)\n",
        "\n",
        "model = Model(input_layer, output_layer)\n",
        "model.compile(optimizer = 'adam', loss= 'sparse_categorical_crossentropy', metrics =['accuracy'])\n",
        "model_history = model.fit(training_images, training_labels,epochs=20, validation_data=(test_images, test_labels),verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10**\n",
        "# **Dimensionality Reduction with PCA and LDA**\n",
        "# **Using Sklearn**"
      ],
      "metadata": {
        "id": "9FfVUKHM-pBn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10.1. Principal Component Analysis**"
      ],
      "metadata": {
        "id": "7FM1iLSg-uvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script 1:"
      ],
      "metadata": {
        "id": "Mv7z2f5R_XYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#imports the required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "NEwg531c_YVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script 2:"
      ],
      "metadata": {
        "id": "pk0ToGp__oKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The following script imports the Iris dataset using the Seaborn library and prints the first five rows of the dataset\n",
        "\n",
        "#importing the dataset\n",
        "iris_df=sns.load_dataset('iris')\n",
        "\n",
        "#print dataset head\n",
        "iris_df.head()"
      ],
      "metadata": {
        "id": "H4NZ1qPx_pZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script 3:"
      ],
      "metadata": {
        "id": "TSxHlWXY_8LT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The following script divides the data into the features and labels sets\n",
        "\n",
        "#creating feature set\n",
        "x=iris_df.drop(['species'],axis=1)\n",
        "\n",
        "#creating label set\n",
        "y=iris_df['species']\n",
        "\n",
        "#converting labels to number\n",
        "from sklearn import preprocessing\n",
        "le=preprocessing.LabelEncoder()\n",
        "y=le.fit_transform(y)"
      ],
      "metadata": {
        "id": "yvJmFNrc_9Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script 4:"
      ],
      "metadata": {
        "id": "STy3TfGgA0Uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Before we apply PCA on a dataset, we will divide it into the training and test sets\n",
        "\n",
        "#dividing data into 80-20% training and test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0 )"
      ],
      "metadata": {
        "id": "usHTpcLiA1OS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script 5:"
      ],
      "metadata": {
        "id": "qR-tk0j_BcQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Finally, both the training and test sets should be scaled before PCA could be applied to them.\n",
        "\n",
        "#applaying scaling on training and test data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc=StandardScaler()\n",
        "x_train=sc.fit_transform(x_train)\n",
        "x_test=sc.fit_transform(x_test)"
      ],
      "metadata": {
        "id": "GlpOWEpsBdcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script 6: To apply PCA via Sklearn, all you have to do is import the\n",
        "PCA class from the Sklearn.decomposition module. Next, to\n",
        "apply PCA to the training set, pass the training set to the fit_\n",
        "tansform() method of the PCA class object. To apply PCA on the test set, pass the test set to the transform() method of the\n",
        "PCA class object. This is shown in the following script."
      ],
      "metadata": {
        "id": "dFnsgBFUCJNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing PCA class\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#creating object of the PCA class\n",
        "pca=PCA()\n",
        "\n",
        "#training PCA model on training data\n",
        "x_train=pca.fit_transform(x_train)\n",
        "\n",
        "#making prediction on test data\n",
        "x_test=pca.transform(x_test)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#print actual datapoint\n",
        "plt.scatter(x_test[:,0],x_test[:,1],c=y_test,cmap='rainbow')"
      ],
      "metadata": {
        "id": "zRn-Vhg_CNH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script 7:"
      ],
      "metadata": {
        "id": "_HVQVDStC2E6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Once you have applied PCA on a dataset, you can use the\n",
        "#explained_variance_ratio_ feature to print variance caused by\n",
        "#all the features in the dataset.\n",
        "\n",
        "#printing variance ratio\n",
        "variance_ratios=pca.explained_variance_ratio_\n",
        "print(variance_ratios)"
      ],
      "metadata": {
        "id": "HSqXpMxGC3VE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output above shows that 72.22 percent of the variance in\n",
        "the dataset is caused by the first principal component, while\n",
        "23.97 percent of the variance is caused by the second principal\n",
        "component.\n",
        "\n",
        "Lets now select the two principal components that caused\n",
        "a collective variance of 96.19 percent (72.22% + 23.97% =\n",
        "96.19%).\n",
        "\n",
        "To select two principal components, all you have to do is pass\n",
        "2 as a value to the n_components attribute of the PCA class.\n",
        "The following script selects two principal components from\n",
        "the Iris training and test sets."
      ],
      "metadata": {
        "id": "eIje58bXDWfi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script 8:"
      ],
      "metadata": {
        "id": "qvpTajt3Db74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#use one principal component\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca=PCA(n_components=2)\n",
        "x_train=pca.fit_transform(x_train)\n",
        "x_test=pca.transform(x_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "G9-1lhBGDfib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets train a classification model using logistic regression,\n",
        "which predicts the label of the iris plant using the two principal\n",
        "components or features, instead of the original four features."
      ],
      "metadata": {
        "id": "zfWul-3YD0L_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script 9:"
      ],
      "metadata": {
        "id": "Ofc6LIsfD0_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#making prediction using logistic regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#training the logistic regression\n",
        "lg=LogisticRegression()\n",
        "lg.fit(x_train,y_train)\n",
        "\n",
        "#predictin the test set results\n",
        "y_pred=lg.predict(x_test)\n",
        "\n",
        "#evaluating results\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(y_test,y_pred))\n",
        "print(y_test)\n",
        "print(y_pred)\n",
        "print(x_test)"
      ],
      "metadata": {
        "id": "gbpHz-j6D16p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output shows that even with two features, the accuracy\n",
        "for correctly predicting the label for the iris plant is 86.66.\n",
        "Finally, with two features, you can easily visualize the dataset\n",
        "using the following script."
      ],
      "metadata": {
        "id": "CAasjaflH_0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Script 10:"
      ],
      "metadata": {
        "id": "kccgyBENIArP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#print actual datapoint\n",
        "plt.scatter(x_test[:,0],_test[:,1],c=y_test,cmap='rainbow')\n"
      ],
      "metadata": {
        "id": "XfMqePbaICHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10.2. Linear Discriminant Analysis**"
      ],
      "metadata": {
        "id": "NWEC6QzeJfVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As always, the first step is to import the required libraries.\n",
        "\n",
        "Script 11:"
      ],
      "metadata": {
        "id": "8anfcOIiJg3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "iIcP16CgJjAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following script imports the dataset and displays its first five rows.\n",
        "\n",
        "\n",
        "Script 12:"
      ],
      "metadata": {
        "id": "JZmjFA72UT7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing dataset\n",
        "banknote_df=pd.read_csv('banknote.csv')\n",
        "\n",
        "#display dataset header\n",
        "banknote_df.head()"
      ],
      "metadata": {
        "id": "o3sr7UtZUWex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets divide the dataset into features and labels.\n",
        "\n",
        "Script 13:"
      ],
      "metadata": {
        "id": "PSBx0FD4UncR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dividing data into features and labels\n",
        "x=banknote_df.drop(['class'],axis=1)\n",
        "y=banknote_df.filter(['class'],axis=1)\n"
      ],
      "metadata": {
        "id": "sD_BYhSwUocx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the following script divides the data into training and test sets.\n",
        "\n",
        "Script 14:"
      ],
      "metadata": {
        "id": "Jcw8JLvfV2uN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dividing data into 80-20% training and test sets\n",
        "from sklearn.model_selection import trains_test_split\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0 )\n"
      ],
      "metadata": {
        "id": "PBPn6HrAV-3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data scaling is performed in the following step.\n",
        "\n",
        "Script 15:"
      ],
      "metadata": {
        "id": "SpNbiXAFWVG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#applying scaling on training and test data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc = StandardScaler()\n",
        "\n",
        "x_train = sc.fit_transform(x_train)\n",
        "x_test = sc.transform (x_test)"
      ],
      "metadata": {
        "id": "eytG5jsgWW0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To apply LDA on the test set, pass the test set to\n",
        "the transform() method of the LDA class object. This is shown in the following script.\n",
        "\n",
        "Script 16:"
      ],
      "metadata": {
        "id": "sc6viuKWWlEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing LDA class\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "\n",
        "#creating object of the LDA class\n",
        "lda = LDA()\n",
        "\n",
        "#training LDA model on training data\n",
        "x_train = lda.fit_transform(x_train, y_train)\n",
        "\n",
        "#making predictions on test data\n",
        "x_test = lda.transform(x_test)"
      ],
      "metadata": {
        "id": "-WaWmC3EWmup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like PCA, you can find variance ratios for LDA using the explained_variance_ratio attribute.\n",
        "\n",
        "Script 17:"
      ],
      "metadata": {
        "id": "P12UwCCBW0_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#printing variance ratios\n",
        "variance_ratios = lda.explained_variance_ratio_\n",
        "print(variance_ratios)"
      ],
      "metadata": {
        "id": "BWY5_OnhW2ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above output shows that even with one component, the maximum variance can be achieved.\n",
        "\n",
        "Next, we select only a single component from our dataset using LDA. To do so, you have to pass 1 as the attribute value for the n_components attribute of the LDA class, as shown below.\n",
        "\n",
        "Script 18:"
      ],
      "metadata": {
        "id": "R9kt7DUEXAP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating object of the LDA class\n",
        "lda = LDA(n_components = 1)\n",
        "\n",
        "#training PCA model on training data\n",
        "x_train = lda.fit_transform(x_train, y_train)\n",
        "\n",
        "#making predictions on test data\n",
        "x_test = lda.transform(x_test)"
      ],
      "metadata": {
        "id": "vBjBSFtxXDQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will try to class whether or not a banknote is fake using a single feature. We will use the LogisticRegression algorithm for that. This is shown in the following script.\n",
        "\n",
        "Script 19:"
      ],
      "metadata": {
        "id": "ACghKIdBXPvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#making predictions using logistic regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#training the logistic regression model\n",
        "lg = LogisticRegression()\n",
        "lg.fit(x_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = lg.predict(x_test)\n",
        "\n",
        "#evaluating results\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "_CC-ncV8XRlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 10.1**\n",
        "\n",
        "**Question 1**\n",
        "\n",
        "Which of the following are the benefits of dimensionality reduction?\n",
        "\n",
        "A. Data Visualization\n",
        "\n",
        "B. Faster training time for statistical algorithms\n",
        "\n",
        "**C. All of the above**\n",
        "\n",
        "D. None of the above\n",
        "\n",
        "\n",
        "**Question 2**\n",
        "\n",
        "In PCA, dimensionality reduction depends upon the:\n",
        "\n",
        "**A. Feature set only**\n",
        "\n",
        "B. Label set only\n",
        "\n",
        "C. Both features and labels sets\n",
        "\n",
        "D. None of the above\n",
        "\n",
        "Question 3\n",
        "\n",
        "LDA is a __________ dimensionality reduction technique.\n",
        "\n",
        "A. Unsupervised\n",
        "\n",
        "B. Semi-Supervised\n",
        "\n",
        "**C. Supervised**\n",
        "\n",
        "D. Reinforcement\n",
        "\n",
        "\n",
        "Exercise 10.2\n",
        "\n",
        "Apply principal component analysis for dimensionality\n",
        "reduction on the customer_churn.csv dataset from the Data folder in the book resources. Print the accuracy using the two principal components. Also, plot the results on the test set using the two principal components."
      ],
      "metadata": {
        "id": "mqOSryyYXlsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "churn_df = pd.read_csv(\"customer_churn.csv\")\n",
        "churn_df.head()"
      ],
      "metadata": {
        "id": "K69UKg37YrWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "churn_df = churn_df.drop(['RowNumber', 'CustomerId','Surname'], axis=1)\n",
        "x = churn_df.drop(['Exited'], axis=1)\n",
        "y = churn_df['Exited']\n",
        "\n",
        "print('x: ',x.head())\n",
        "print('y: ',y.head())"
      ],
      "metadata": {
        "id": "3MufioacY8BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical = x.drop(['Geography', 'Gender'], axis = 1)\n",
        "categorical = x.filter(['Geography', 'Gender'])\n",
        "\n",
        "cat_numerical = pd.get_dummies(categorical,drop_first=True)\n",
        "x = pd.concat([numerical, cat_numerical], axis = 1)\n",
        "x.head()"
      ],
      "metadata": {
        "id": "LNI49KfrZSQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.20, random_state=0)\n",
        "\n",
        "#applying scaling on training and test data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "x_train = sc.fit_transform(x_train)\n",
        "xtest = sc.transform (x_test)"
      ],
      "metadata": {
        "id": "Fb4Ee4QgZfwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing PCA class\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#creating object of the PCA class\n",
        "pca = PCA()\n",
        "\n",
        "#training PCA model on training data\n",
        "x_train = pca.fit_transform(x_train)\n",
        "\n",
        "#making predictions on test data\n",
        "x_test = pca.transform(x_test)\n",
        "\n",
        "#printing variance ratios\n",
        "variance_ratios = pca.explained_variance_ratio_\n",
        "print(variance_ratios)"
      ],
      "metadata": {
        "id": "bpCYay73Zs2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use one principal component\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "x_train = pca.fit_transform(x_train)\n",
        "x_test = pca.transform(x_test)\n",
        "\n",
        "#making predictions using logistic regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#training the logistic regression model\n",
        "lg = LogisticRegression()\n",
        "lg.fit(x_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = lg.predict(x_test)\n",
        "\n",
        "#evaluating results\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "print(accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "RstA6nunaF03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#print actual datapoints\n",
        "\n",
        "plt.scatter(x_test[:,0], x_test[:,1], c= y_test, cmap='rainbow' )"
      ],
      "metadata": {
        "id": "MNLZyp7nYQ6y"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}